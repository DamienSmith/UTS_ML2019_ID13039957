{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of Untitled2.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DamienSmith/UTS_ML2019_ID13039957/blob/master/A2_PracticalProject_13039957_13026998.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TWf4FTWL7mhY",
        "colab_type": "text"
      },
      "source": [
        "# A2: Practical Machine Learning Project | 31005 | Advanced Data Analytics\n",
        "Student: 13039957 & 13026998\n",
        "\n",
        "Link to Github:\n",
        "https://github.com/DamienSmith/UTS_ML2019_ID13039957/blob/master/A2_PracticalProject_13039957_13026998.ipynb"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N0-ciK386-52",
        "colab_type": "text"
      },
      "source": [
        "##Introduction (100)\n",
        "\n",
        "Project overview.\n",
        "\n",
        "**Discuss the Algorithm** \n",
        "* Linear regression or decision tree\n",
        "\n",
        "There a various types of decision tree algorithms, which we attempt to build and compare. The different type that are dicussed are:\n",
        "\n",
        "**1. ID3**\n",
        "\n",
        "ID3 (Iterative Dichotomiser 3) was developed in 1986 by Ross Quinlan. The algorithm creates a multiway tree, finding for each node (i.e. in a greedy manner) the categorical feature that will yield the largest information gain for categorical targets. Trees are grown to their maximum size and then a pruning step is usually applied to improve the ability of the tree to generalise to unseen data.\n",
        "\n",
        "\n",
        "**2. C4.5**\n",
        "\n",
        "C4.5 is the successor to ID3 and removed the restriction that features must be categorical by dynamically defining a discrete attribute (based on numerical variables) that partitions the continuous attribute value into a discrete set of intervals. C4.5 converts the trained trees (i.e. the output of the ID3 algorithm) into sets of if-then rules. This accuracy of each rule is then evaluated to determine the order in which they should be applied. Pruning is done by removing a rule’s precondition if the accuracy of the rule improves without it.\n",
        "\n",
        "**3. CART**\n",
        "\n",
        "CART (Classification and Regression Trees) is very similar to C4.5, but it differs in that it supports numerical target variables (regression) and does not compute rule sets. CART constructs binary trees using the feature and threshold that yields the largest information gain at each node.\n",
        "\n",
        "\n",
        "**Define Input/Output** \n",
        "* What data you use and what you’re getting out\n",
        "* the format of the I/O data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-076P8Yx6-iu",
        "colab_type": "text"
      },
      "source": [
        "##Exploration (300)\n",
        "\n",
        "**Identify Challenges**\n",
        "\n",
        "* Highlight the practical significance of the project\n",
        "* What problems exist and how to manage them\n",
        "  * e.g. memory management, time efficiency, 'advanced functions' (eg parallelism) \n",
        "\n",
        "**Design Data Structures**\n",
        "\n",
        "* Design/planning of research/development is clear and logical\n",
        "* Cover these:\n",
        "  * data acquisition\n",
        "  * quality control\n",
        "  * modelling technicques\n",
        "  * evaluation method & criteria\n",
        "\n",
        "**Plan Data Models and Tests**\n",
        "\n",
        "* Logical design (correct, efficient and practically complete)\n",
        "* Evaluation method/Testing (Compare and consider alternatives)\n",
        "* Must be able to run code through collab \n",
        "  * i.e. data load and python libraries need to work in Collab\n",
        "\n",
        "\n",
        "**Possible alternatives**\n",
        "\n",
        "* Algorithm Tuning. The application of CART to the Bank Note dataset was not tuned. Experiment with different parameter values and see if you can achieve better performance.\n",
        "\n",
        "* Cross Entropy. Another cost function for evaluating splits is cross entropy (logloss). You could implement and experiment with this alternative cost function.\n",
        "\n",
        "* Tree Pruning. An important technique for reducing overfitting of the training dataset is to prune the trees. Investigate and implement tree pruning methods.\n",
        "\n",
        "* Categorical Dataset. The example was designed for input data with numerical or ordinal input attributes, experiment with categorical input data and splits that may use equality instead of ranking.\n",
        "\n",
        "* Regression. Adapt the tree for regression using a different cost function and method for creating terminal nodes.\n",
        "\n",
        "* More Datasets. Apply the algorithm to more datasets on the UCI Machine Learning Repository.\n",
        "\n",
        "\n",
        "\n",
        "There several choices there is historically ID3 the Iterative Dichotomiser 3, C5 which I believe stands for Classifier as development of ID3. CART, which is for classification and regression tree. It's very common in Universal Choice. CHAID, which is for CHi-squared Automatic Interaction Detectior. MARS, which is multivariate adaptive regression splines. And the one that I'm going to be using Conditional Inference trees. Now decision trees in general have some pros and cons.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dv3kz8Cx6-bm",
        "colab_type": "text"
      },
      "source": [
        "##Methodology (100 ex comments)\n",
        "\n",
        "[code here]\n",
        "\n",
        "**Build and Train Data Models**\n",
        "\n",
        "* Comments connect the code to the algorithm steps\n",
        "* \n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JuFQehMoR0uz",
        "colab_type": "text"
      },
      "source": [
        "## Decision Tree Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P9F55nV5LWxV",
        "colab_type": "code",
        "outputId": "432ebb35-4b1f-4cbf-c2ee-b5cc2c921844",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "## \n",
        "##    A2: Practical Machine Learning Project | 31005 | Advanced Data Analytics\n",
        "##    \n",
        "##    Authors: Rae Ho (13026998) & Damien Smith (13039957)\n",
        "##    Goals: - Implement Decision Tree Algorithm\n",
        "##           - Build & Train a model\n",
        "##           - Explore/Compare alternative options within the algorithm\n",
        "##    Code: Python 3\n",
        "##    Github: https://github.com/DamienSmith/UTS_ML2019_ID13039957/blob/master/A2_PracticalProject_13039957_13026998.ipynb\n",
        "##\n",
        "##\n",
        "\n",
        "# Import Libraries\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import pandas_profiling as pp\n",
        "from sklearn import datasets\n",
        "\n",
        "# Load Data\n",
        "# Just some test data\n",
        "dataset = datasets.load_iris()\n",
        "X = dataset.data[:, [0, 2]]\n",
        "y = dataset.target\n",
        "dataset\n",
        "# Get dataset from github\n",
        "# url = 'https://raw.githubusercontent.com/DamienSmith/UTS_ML2019_ID13039957/master/characters.csv'\n",
        "# df = pd.read_csv(url)\n",
        "\n",
        "#Data profiling\n",
        "#pp.ProfileReport(dataset)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'DESCR': '.. _iris_dataset:\\n\\nIris plants dataset\\n--------------------\\n\\n**Data Set Characteristics:**\\n\\n    :Number of Instances: 150 (50 in each of three classes)\\n    :Number of Attributes: 4 numeric, predictive attributes and the class\\n    :Attribute Information:\\n        - sepal length in cm\\n        - sepal width in cm\\n        - petal length in cm\\n        - petal width in cm\\n        - class:\\n                - Iris-Setosa\\n                - Iris-Versicolour\\n                - Iris-Virginica\\n                \\n    :Summary Statistics:\\n\\n    ============== ==== ==== ======= ===== ====================\\n                    Min  Max   Mean    SD   Class Correlation\\n    ============== ==== ==== ======= ===== ====================\\n    sepal length:   4.3  7.9   5.84   0.83    0.7826\\n    sepal width:    2.0  4.4   3.05   0.43   -0.4194\\n    petal length:   1.0  6.9   3.76   1.76    0.9490  (high!)\\n    petal width:    0.1  2.5   1.20   0.76    0.9565  (high!)\\n    ============== ==== ==== ======= ===== ====================\\n\\n    :Missing Attribute Values: None\\n    :Class Distribution: 33.3% for each of 3 classes.\\n    :Creator: R.A. Fisher\\n    :Donor: Michael Marshall (MARSHALL%PLU@io.arc.nasa.gov)\\n    :Date: July, 1988\\n\\nThe famous Iris database, first used by Sir R.A. Fisher. The dataset is taken\\nfrom Fisher\\'s paper. Note that it\\'s the same as in R, but not as in the UCI\\nMachine Learning Repository, which has two wrong data points.\\n\\nThis is perhaps the best known database to be found in the\\npattern recognition literature.  Fisher\\'s paper is a classic in the field and\\nis referenced frequently to this day.  (See Duda & Hart, for example.)  The\\ndata set contains 3 classes of 50 instances each, where each class refers to a\\ntype of iris plant.  One class is linearly separable from the other 2; the\\nlatter are NOT linearly separable from each other.\\n\\n.. topic:: References\\n\\n   - Fisher, R.A. \"The use of multiple measurements in taxonomic problems\"\\n     Annual Eugenics, 7, Part II, 179-188 (1936); also in \"Contributions to\\n     Mathematical Statistics\" (John Wiley, NY, 1950).\\n   - Duda, R.O., & Hart, P.E. (1973) Pattern Classification and Scene Analysis.\\n     (Q327.D83) John Wiley & Sons.  ISBN 0-471-22361-1.  See page 218.\\n   - Dasarathy, B.V. (1980) \"Nosing Around the Neighborhood: A New System\\n     Structure and Classification Rule for Recognition in Partially Exposed\\n     Environments\".  IEEE Transactions on Pattern Analysis and Machine\\n     Intelligence, Vol. PAMI-2, No. 1, 67-71.\\n   - Gates, G.W. (1972) \"The Reduced Nearest Neighbor Rule\".  IEEE Transactions\\n     on Information Theory, May 1972, 431-433.\\n   - See also: 1988 MLC Proceedings, 54-64.  Cheeseman et al\"s AUTOCLASS II\\n     conceptual clustering system finds 3 classes in the data.\\n   - Many, many more ...',\n",
              " 'data': array([[5.1, 3.5, 1.4, 0.2],\n",
              "        [4.9, 3. , 1.4, 0.2],\n",
              "        [4.7, 3.2, 1.3, 0.2],\n",
              "        [4.6, 3.1, 1.5, 0.2],\n",
              "        [5. , 3.6, 1.4, 0.2],\n",
              "        [5.4, 3.9, 1.7, 0.4],\n",
              "        [4.6, 3.4, 1.4, 0.3],\n",
              "        [5. , 3.4, 1.5, 0.2],\n",
              "        [4.4, 2.9, 1.4, 0.2],\n",
              "        [4.9, 3.1, 1.5, 0.1],\n",
              "        [5.4, 3.7, 1.5, 0.2],\n",
              "        [4.8, 3.4, 1.6, 0.2],\n",
              "        [4.8, 3. , 1.4, 0.1],\n",
              "        [4.3, 3. , 1.1, 0.1],\n",
              "        [5.8, 4. , 1.2, 0.2],\n",
              "        [5.7, 4.4, 1.5, 0.4],\n",
              "        [5.4, 3.9, 1.3, 0.4],\n",
              "        [5.1, 3.5, 1.4, 0.3],\n",
              "        [5.7, 3.8, 1.7, 0.3],\n",
              "        [5.1, 3.8, 1.5, 0.3],\n",
              "        [5.4, 3.4, 1.7, 0.2],\n",
              "        [5.1, 3.7, 1.5, 0.4],\n",
              "        [4.6, 3.6, 1. , 0.2],\n",
              "        [5.1, 3.3, 1.7, 0.5],\n",
              "        [4.8, 3.4, 1.9, 0.2],\n",
              "        [5. , 3. , 1.6, 0.2],\n",
              "        [5. , 3.4, 1.6, 0.4],\n",
              "        [5.2, 3.5, 1.5, 0.2],\n",
              "        [5.2, 3.4, 1.4, 0.2],\n",
              "        [4.7, 3.2, 1.6, 0.2],\n",
              "        [4.8, 3.1, 1.6, 0.2],\n",
              "        [5.4, 3.4, 1.5, 0.4],\n",
              "        [5.2, 4.1, 1.5, 0.1],\n",
              "        [5.5, 4.2, 1.4, 0.2],\n",
              "        [4.9, 3.1, 1.5, 0.2],\n",
              "        [5. , 3.2, 1.2, 0.2],\n",
              "        [5.5, 3.5, 1.3, 0.2],\n",
              "        [4.9, 3.6, 1.4, 0.1],\n",
              "        [4.4, 3. , 1.3, 0.2],\n",
              "        [5.1, 3.4, 1.5, 0.2],\n",
              "        [5. , 3.5, 1.3, 0.3],\n",
              "        [4.5, 2.3, 1.3, 0.3],\n",
              "        [4.4, 3.2, 1.3, 0.2],\n",
              "        [5. , 3.5, 1.6, 0.6],\n",
              "        [5.1, 3.8, 1.9, 0.4],\n",
              "        [4.8, 3. , 1.4, 0.3],\n",
              "        [5.1, 3.8, 1.6, 0.2],\n",
              "        [4.6, 3.2, 1.4, 0.2],\n",
              "        [5.3, 3.7, 1.5, 0.2],\n",
              "        [5. , 3.3, 1.4, 0.2],\n",
              "        [7. , 3.2, 4.7, 1.4],\n",
              "        [6.4, 3.2, 4.5, 1.5],\n",
              "        [6.9, 3.1, 4.9, 1.5],\n",
              "        [5.5, 2.3, 4. , 1.3],\n",
              "        [6.5, 2.8, 4.6, 1.5],\n",
              "        [5.7, 2.8, 4.5, 1.3],\n",
              "        [6.3, 3.3, 4.7, 1.6],\n",
              "        [4.9, 2.4, 3.3, 1. ],\n",
              "        [6.6, 2.9, 4.6, 1.3],\n",
              "        [5.2, 2.7, 3.9, 1.4],\n",
              "        [5. , 2. , 3.5, 1. ],\n",
              "        [5.9, 3. , 4.2, 1.5],\n",
              "        [6. , 2.2, 4. , 1. ],\n",
              "        [6.1, 2.9, 4.7, 1.4],\n",
              "        [5.6, 2.9, 3.6, 1.3],\n",
              "        [6.7, 3.1, 4.4, 1.4],\n",
              "        [5.6, 3. , 4.5, 1.5],\n",
              "        [5.8, 2.7, 4.1, 1. ],\n",
              "        [6.2, 2.2, 4.5, 1.5],\n",
              "        [5.6, 2.5, 3.9, 1.1],\n",
              "        [5.9, 3.2, 4.8, 1.8],\n",
              "        [6.1, 2.8, 4. , 1.3],\n",
              "        [6.3, 2.5, 4.9, 1.5],\n",
              "        [6.1, 2.8, 4.7, 1.2],\n",
              "        [6.4, 2.9, 4.3, 1.3],\n",
              "        [6.6, 3. , 4.4, 1.4],\n",
              "        [6.8, 2.8, 4.8, 1.4],\n",
              "        [6.7, 3. , 5. , 1.7],\n",
              "        [6. , 2.9, 4.5, 1.5],\n",
              "        [5.7, 2.6, 3.5, 1. ],\n",
              "        [5.5, 2.4, 3.8, 1.1],\n",
              "        [5.5, 2.4, 3.7, 1. ],\n",
              "        [5.8, 2.7, 3.9, 1.2],\n",
              "        [6. , 2.7, 5.1, 1.6],\n",
              "        [5.4, 3. , 4.5, 1.5],\n",
              "        [6. , 3.4, 4.5, 1.6],\n",
              "        [6.7, 3.1, 4.7, 1.5],\n",
              "        [6.3, 2.3, 4.4, 1.3],\n",
              "        [5.6, 3. , 4.1, 1.3],\n",
              "        [5.5, 2.5, 4. , 1.3],\n",
              "        [5.5, 2.6, 4.4, 1.2],\n",
              "        [6.1, 3. , 4.6, 1.4],\n",
              "        [5.8, 2.6, 4. , 1.2],\n",
              "        [5. , 2.3, 3.3, 1. ],\n",
              "        [5.6, 2.7, 4.2, 1.3],\n",
              "        [5.7, 3. , 4.2, 1.2],\n",
              "        [5.7, 2.9, 4.2, 1.3],\n",
              "        [6.2, 2.9, 4.3, 1.3],\n",
              "        [5.1, 2.5, 3. , 1.1],\n",
              "        [5.7, 2.8, 4.1, 1.3],\n",
              "        [6.3, 3.3, 6. , 2.5],\n",
              "        [5.8, 2.7, 5.1, 1.9],\n",
              "        [7.1, 3. , 5.9, 2.1],\n",
              "        [6.3, 2.9, 5.6, 1.8],\n",
              "        [6.5, 3. , 5.8, 2.2],\n",
              "        [7.6, 3. , 6.6, 2.1],\n",
              "        [4.9, 2.5, 4.5, 1.7],\n",
              "        [7.3, 2.9, 6.3, 1.8],\n",
              "        [6.7, 2.5, 5.8, 1.8],\n",
              "        [7.2, 3.6, 6.1, 2.5],\n",
              "        [6.5, 3.2, 5.1, 2. ],\n",
              "        [6.4, 2.7, 5.3, 1.9],\n",
              "        [6.8, 3. , 5.5, 2.1],\n",
              "        [5.7, 2.5, 5. , 2. ],\n",
              "        [5.8, 2.8, 5.1, 2.4],\n",
              "        [6.4, 3.2, 5.3, 2.3],\n",
              "        [6.5, 3. , 5.5, 1.8],\n",
              "        [7.7, 3.8, 6.7, 2.2],\n",
              "        [7.7, 2.6, 6.9, 2.3],\n",
              "        [6. , 2.2, 5. , 1.5],\n",
              "        [6.9, 3.2, 5.7, 2.3],\n",
              "        [5.6, 2.8, 4.9, 2. ],\n",
              "        [7.7, 2.8, 6.7, 2. ],\n",
              "        [6.3, 2.7, 4.9, 1.8],\n",
              "        [6.7, 3.3, 5.7, 2.1],\n",
              "        [7.2, 3.2, 6. , 1.8],\n",
              "        [6.2, 2.8, 4.8, 1.8],\n",
              "        [6.1, 3. , 4.9, 1.8],\n",
              "        [6.4, 2.8, 5.6, 2.1],\n",
              "        [7.2, 3. , 5.8, 1.6],\n",
              "        [7.4, 2.8, 6.1, 1.9],\n",
              "        [7.9, 3.8, 6.4, 2. ],\n",
              "        [6.4, 2.8, 5.6, 2.2],\n",
              "        [6.3, 2.8, 5.1, 1.5],\n",
              "        [6.1, 2.6, 5.6, 1.4],\n",
              "        [7.7, 3. , 6.1, 2.3],\n",
              "        [6.3, 3.4, 5.6, 2.4],\n",
              "        [6.4, 3.1, 5.5, 1.8],\n",
              "        [6. , 3. , 4.8, 1.8],\n",
              "        [6.9, 3.1, 5.4, 2.1],\n",
              "        [6.7, 3.1, 5.6, 2.4],\n",
              "        [6.9, 3.1, 5.1, 2.3],\n",
              "        [5.8, 2.7, 5.1, 1.9],\n",
              "        [6.8, 3.2, 5.9, 2.3],\n",
              "        [6.7, 3.3, 5.7, 2.5],\n",
              "        [6.7, 3. , 5.2, 2.3],\n",
              "        [6.3, 2.5, 5. , 1.9],\n",
              "        [6.5, 3. , 5.2, 2. ],\n",
              "        [6.2, 3.4, 5.4, 2.3],\n",
              "        [5.9, 3. , 5.1, 1.8]]),\n",
              " 'feature_names': ['sepal length (cm)',\n",
              "  'sepal width (cm)',\n",
              "  'petal length (cm)',\n",
              "  'petal width (cm)'],\n",
              " 'filename': '/usr/local/lib/python3.6/dist-packages/sklearn/datasets/data/iris.csv',\n",
              " 'target': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
              "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
              "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]),\n",
              " 'target_names': array(['setosa', 'versicolor', 'virginica'], dtype='<U10')}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lXdOwk-6QICD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xv4S9QNXQsBJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Define Gini_index() Function \n",
        "# Calculate the Gini index for a split dataset\n",
        "\n",
        "def gini_index(groups, classes):\n",
        "\t# count all samples at split point\n",
        "\tn_instances = float(sum([len(group) for group in groups]))\n",
        "\t# sum weighted Gini index for each group\n",
        "\tgini = 0.0\n",
        "\tfor group in groups:\n",
        "\t\tsize = float(len(group))\n",
        "\t\t# avoid divide by zero\n",
        "\t\tif size == 0:\n",
        "\t\t\tcontinue\n",
        "\t\tscore = 0.0\n",
        "\t\t# score the group based on the score for each class\n",
        "\t\tfor class_val in classes:\n",
        "\t\t\tp = [row[-1] for row in group].count(class_val) / size\n",
        "\t\t\tscore += p * p\n",
        "\t\t# weight the group score by its relative size\n",
        "\t\tgini += (1.0 - score) * (size / n_instances)\n",
        "\treturn gini\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ShJDI7lFQuDZ",
        "colab_type": "code",
        "outputId": "5d4ca3a8-5aa1-4574-900b-dc14a75da1f7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "# Define worst_gini_index_test() Function\n",
        "# Test on worst-case where there is a 50/50 split on each group\n",
        "# For a comparison\n",
        "\n",
        "def worst_gini_index_test(groups, classes):\n",
        "\t# count all samples at split point\n",
        "\tn_instances = float(sum([len(group) for group in groups]))\n",
        "\t# sum weighted Gini index for each group\n",
        "\tgini = 0.0\n",
        "\tfor group in groups:\n",
        "\t\tsize = float(len(group))\n",
        "\t\t# avoid divide by zero\n",
        "\t\tif size == 0:\n",
        "\t\t\tcontinue\n",
        "\t\tscore = 0.0\n",
        "\t\t# score the group based on the score for each class\n",
        "\t\tfor class_val in classes:\n",
        "\t\t\tp = [row[-1] for row in group].count(class_val) / size\n",
        "\t\t\tscore += p * p\n",
        "\t\t# weight the group score by its relative size\n",
        "\t\tgini += (1.0 - score) * (size / n_instances)\n",
        "\treturn gini\n",
        " \n",
        "# test Gini values\n",
        "print(gini_index([[[1, 1], [1, 0]], [[1, 1], [1, 0]]], [0, 1]))\n",
        "print(gini_index([[[1, 0], [1, 0]], [[1, 1], [1, 1]]], [0, 1]))\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.5\n",
            "0.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-JyFVr7NRfop",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Define test_split() Function.\n",
        "# Separate the dataset into two lists of rows given the index of an attribute and a split value for that attribute.\n",
        "# Once we have the two groups, we can then use our Gini score above to evaluate the cost of the split.\n",
        "# Note: The right group contains all rows with a value at the index above or equal to the split value.\n",
        "\n",
        "# Split a dataset based on an attribute and an attribute value\n",
        "def test_split(index, value, dataset):\n",
        "\tleft, right = list(), list()\n",
        "\tfor row in dataset:\n",
        "\t\tif row[index] < value:\n",
        "\t\t\tleft.append(row)\n",
        "\t\telse:\n",
        "\t\t\tright.append(row)\n",
        "\treturn left, right\n",
        "\n",
        "\n",
        "# Splitting a dataset involves iterating over each row, checking if the attribute value is below or above the split value and assigning it to the left or right group respectively."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iy5EjwWHSKzZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Define get_split() Function.\n",
        "# With the Gini function above and the test split function we now have everything we need to evaluate splits.\n",
        "# Given a dataset, we must check every value on each attribute as a candidate split, evaluate the cost of the split and find the best possible split we could make.\n",
        "# Once the best split is found, we can use it as a node in our decision tree\n",
        "\n",
        "def get_split(dataset):\n",
        "\tclass_values = list(set(row[-1] for row in dataset))\n",
        "\tb_index, b_value, b_score, b_groups = 999, 999, 999, None\n",
        "\tfor index in range(len(dataset[0])-1):\n",
        "\t\tfor row in dataset:\n",
        "\t\t\tgroups = test_split(index, row[index], dataset)\n",
        "\t\t\tgini = gini_index(groups, class_values)\n",
        "\t\t\tprint('X%d < %.3f Gini=%.3f' % ((index+1), row[index], gini))\n",
        "\t\t\tif gini < b_score:\n",
        "\t\t\t\tb_index, b_value, b_score, b_groups = index, row[index], gini, groups\n",
        "\treturn {'index':b_index, 'value':b_value, 'groups':b_groups}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oO8C9l0hXOSL",
        "colab_type": "code",
        "outputId": "84a2fe66-13bf-408d-c4b5-578ec9a2929e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        }
      },
      "source": [
        "# Calculate the Gini index for a split dataset\n",
        "def gini_index(groups, classes):\n",
        "\t# count all samples at split point\n",
        "\tn_instances = float(sum([len(group) for group in groups]))\n",
        "\t# sum weighted Gini index for each group\n",
        "\tgini = 0.0\n",
        "\tfor group in groups:\n",
        "\t\tsize = float(len(group))\n",
        "\t\t# avoid divide by zero\n",
        "\t\tif size == 0:\n",
        "\t\t\tcontinue\n",
        "\t\tscore = 0.0\n",
        "\t\t# score the group based on the score for each class\n",
        "\t\tfor class_val in classes:\n",
        "\t\t\tp = [row[-1] for row in group].count(class_val) / size\n",
        "\t\t\tscore += p * p\n",
        "\t\t# weight the group score by its relative size\n",
        "\t\tgini += (1.0 - score) * (size / n_instances)\n",
        "\treturn gini\n",
        "\n",
        "# Split a dataset based on an attribute and an attribute value\n",
        "def test_split(index, value, dataset):\n",
        "\tleft, right = list(), list()\n",
        "\tfor row in dataset:\n",
        "\t\tif row[index] < value:\n",
        "\t\t\tleft.append(row)\n",
        "\t\telse:\n",
        "\t\t\tright.append(row)\n",
        "\treturn left, right\n",
        "\n",
        "# Select the best split point for a dataset\n",
        "def get_split(dataset):\n",
        "\tclass_values = list(set(row[-1] for row in dataset))\n",
        "\tb_index, b_value, b_score, b_groups = 999, 999, 999, None\n",
        "\tfor index in range(len(dataset[0])-1):\n",
        "\t\tfor row in dataset:\n",
        "\t\t\tgroups = test_split(index, row[index], dataset)\n",
        "\t\t\tgini = gini_index(groups, class_values)\n",
        "\t\t\tprint('X%d < %.3f Gini=%.3f' % ((index+1), row[index], gini))\n",
        "\t\t\tif gini < b_score:\n",
        "\t\t\t\tb_index, b_value, b_score, b_groups = index, row[index], gini, groups\n",
        "\treturn {'index':b_index, 'value':b_value, 'groups':b_groups}\n",
        " \n",
        "split = get_split(dataset)\n",
        "print('Split: [X%d < %.3f]' % ((split['index']+1), split['value']))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "X1 < 2.771 Gini=0.444\n",
            "X1 < 1.729 Gini=0.500\n",
            "X1 < 3.678 Gini=0.286\n",
            "X1 < 3.961 Gini=0.167\n",
            "X1 < 2.999 Gini=0.375\n",
            "X1 < 7.498 Gini=0.286\n",
            "X1 < 9.002 Gini=0.375\n",
            "X1 < 7.445 Gini=0.167\n",
            "X1 < 10.125 Gini=0.444\n",
            "X1 < 6.642 Gini=0.000\n",
            "X2 < 1.785 Gini=0.500\n",
            "X2 < 1.170 Gini=0.444\n",
            "X2 < 2.813 Gini=0.320\n",
            "X2 < 2.620 Gini=0.417\n",
            "X2 < 2.209 Gini=0.476\n",
            "X2 < 3.163 Gini=0.167\n",
            "X2 < 3.339 Gini=0.444\n",
            "X2 < 0.477 Gini=0.500\n",
            "X2 < 3.235 Gini=0.286\n",
            "X2 < 3.320 Gini=0.375\n",
            "Split: [X1 < 6.642]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8YGS3CuVZD4G",
        "colab_type": "code",
        "outputId": "3d0fa00c-e3e2-4d9d-a520-473c11971fd3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281
        }
      },
      "source": [
        "# plot scatter plot - needs work\n",
        "x = [row[0] for row in dataset]\n",
        "y = [row[1] for row in dataset]\n",
        "\n",
        "data = x,y\n",
        "colors = (\"red\", \"blue\")\n",
        "groups = (\"X1\", \"X2\")\n",
        "\n",
        "# Create plot\n",
        "fig = plt.figure()\n",
        "ax = fig.add_subplot(1, 1, 1)\n",
        "\n",
        "for data, color, group in zip(data, colors, groups):\n",
        "  x = data\n",
        "  y = data\n",
        "  ax.scatter(x, y, alpha=0.8, c=color, edgecolors='none', s=30, label=group)\n",
        "\n",
        "plt.title('Scatter plot')\n",
        "plt.legend(loc=2)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAEICAYAAABPgw/pAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n/wGWoQkhyhDQzQAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W8Vaj5HThPEB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# We need to control the size of a tree by defining the depth and the number of rows that a node will run.\n",
        "\n",
        "# We use user-defined arguments to define tree building procedure.\n",
        "# Maximum Tree Depth. This is the maximum number of nodes from the root node of the tree. Once a maximum depth of the tree is met, we must stop splitting adding new nodes. Deeper trees are more complex and are more likely to overfit the training data.\n",
        "# Minimum Node Records. This is the minimum number of training patterns that a given node is responsible for. Once at or below this minimum, we must stop splitting and adding new nodes. Nodes that account for too few training patterns are expected to be too specific and are likely to overfit the training data.\n",
        "\n",
        "# There is one more condition. It is possible to choose a split in which all rows belong to one group. In this case, we will be unable to continue splitting and adding child nodes as we will have no records to split on one side or another.\n",
        "\n",
        "# Now we have some ideas of when to stop growing the tree. When we do stop growing at a given point, that node is called a terminal node and is used to make a final prediction.\n",
        "\n",
        "# This is done by taking the group of rows assigned to that node and selecting the most common class value in the group. This will be used to make predictions.\n",
        "\n",
        "\n",
        "# Define to_terminal() Function\n",
        "# Select a class value for a group of rows. It returns the most common output value in a list of rows.\n",
        "def to_terminal(group):\n",
        "\toutcomes = [row[-1] for row in group]\n",
        "\treturn max(set(outcomes), key=outcomes.count)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-FkenWeghnVf",
        "colab_type": "code",
        "outputId": "69d9c300-1cf6-4bc4-bf5b-5edf51289608",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "# # Split a dataset based on an attribute and an attribute value\n",
        "def test_split(index, value, dataset):\n",
        "\tleft, right = list(), list()\n",
        "\tfor row in dataset:\n",
        "\t\tif row[index] < value:\n",
        "\t\t\tleft.append(row)\n",
        "\t\telse:\n",
        "\t\t\tright.append(row)\n",
        "\treturn left, right\n",
        " \n",
        "# Calculate the Gini index for a split dataset\n",
        "def gini_index(groups, classes):\n",
        "\t# count all samples at split point\n",
        "\tn_instances = float(sum([len(group) for group in groups]))\n",
        "\t# sum weighted Gini index for each group\n",
        "\tgini = 0.0\n",
        "\tfor group in groups:\n",
        "\t\tsize = float(len(group))\n",
        "\t\t# avoid divide by zero\n",
        "\t\tif size == 0:\n",
        "\t\t\tcontinue\n",
        "\t\tscore = 0.0\n",
        "\t\t# score the group based on the score for each class\n",
        "\t\tfor class_val in classes:\n",
        "\t\t\tp = [row[-1] for row in group].count(class_val) / size\n",
        "\t\t\tscore += p * p\n",
        "\t\t# weight the group score by its relative size\n",
        "\t\tgini += (1.0 - score) * (size / n_instances)\n",
        "\treturn gini\n",
        " \n",
        "# Select the best split point for a dataset\n",
        "def get_split(dataset):\n",
        "\tclass_values = list(set(row[-1] for row in dataset))\n",
        "\tb_index, b_value, b_score, b_groups = 999, 999, 999, None\n",
        "\tfor index in range(len(dataset[0])-1):\n",
        "\t\tfor row in dataset:\n",
        "\t\t\tgroups = test_split(index, row[index], dataset)\n",
        "\t\t\tgini = gini_index(groups, class_values)\n",
        "\t\t\tif gini < b_score:\n",
        "\t\t\t\tb_index, b_value, b_score, b_groups = index, row[index], gini, groups\n",
        "\treturn {'index':b_index, 'value':b_value, 'groups':b_groups}\n",
        " \n",
        "# Create a terminal node value\n",
        "def to_terminal(group):\n",
        "\toutcomes = [row[-1] for row in group]\n",
        "\treturn max(set(outcomes), key=outcomes.count)\n",
        " \n",
        "# Create child splits for a node or make terminal\n",
        "def split(node, max_depth, min_size, depth):\n",
        "\tleft, right = node['groups']\n",
        "\tdel(node['groups'])\n",
        "\t# check for a no split\n",
        "\tif not left or not right:\n",
        "\t\tnode['left'] = node['right'] = to_terminal(left + right)\n",
        "\t\treturn\n",
        "\t# check for max depth\n",
        "\tif depth >= max_depth:\n",
        "\t\tnode['left'], node['right'] = to_terminal(left), to_terminal(right)\n",
        "\t\treturn\n",
        "\t# process left child\n",
        "\tif len(left) <= min_size:\n",
        "\t\tnode['left'] = to_terminal(left)\n",
        "\telse:\n",
        "\t\tnode['left'] = get_split(left)\n",
        "\t\tsplit(node['left'], max_depth, min_size, depth+1)\n",
        "\t# process right child\n",
        "\tif len(right) <= min_size:\n",
        "\t\tnode['right'] = to_terminal(right)\n",
        "\telse:\n",
        "\t\tnode['right'] = get_split(right)\n",
        "\t\tsplit(node['right'], max_depth, min_size, depth+1)\n",
        "    \n",
        "# Build a decision tree\n",
        "def build_tree(train, max_depth, min_size):\n",
        "\troot = get_split(train)\n",
        "\tsplit(root, max_depth, min_size, 1)\n",
        "\treturn root\n",
        " \n",
        "# Print a decision tree\n",
        "def print_tree(node, depth=0):\n",
        "\tif isinstance(node, dict):\n",
        "\t\tprint('%s[X%d < %.3f]' % ((depth*' ', (node['index']+1), node['value'])))\n",
        "\t\tprint_tree(node['left'], depth+1)\n",
        "\t\tprint_tree(node['right'], depth+1)\n",
        "\telse:\n",
        "\t\tprint('%s[%s]' % ((depth*' ', node)))\n",
        "\n",
        "tree = build_tree(dataset, 1, 1)\n",
        "print_tree(tree)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[X1 < 6.642]\n",
            " [0]\n",
            " [1]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e4yODkiVhnO9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## show some experimenting with different 'max depth' parameters\n",
        "\n",
        "##\n",
        "\n",
        "##\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8dg4c-2LiL2P",
        "colab_type": "code",
        "outputId": "56f6ebf8-2a7a-4108-e4f9-bbb4d73afa21",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "source": [
        "# Make a prediction with a decision tree\n",
        "def predict(node, row):\n",
        "\tif row[node['index']] < node['value']:\n",
        "\t\tif isinstance(node['left'], dict):\n",
        "\t\t\treturn predict(node['left'], row)\n",
        "\t\telse:\n",
        "\t\t\treturn node['left']\n",
        "\telse:\n",
        "\t\tif isinstance(node['right'], dict):\n",
        "\t\t\treturn predict(node['right'], row)\n",
        "\t\telse:\n",
        "\t\t\treturn node['right']\n",
        "    \n",
        "#  predict with a stump\n",
        "stump = {'index': 0, 'right': 1, 'value': 6.642287351, 'left': 0}\n",
        "for row in dataset:\n",
        "\tprediction = predict(stump, row)\n",
        "\tprint('Predicted=%d, Result=%d' % (row[-1], prediction))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Predicted=0, Result=0\n",
            "Predicted=0, Result=0\n",
            "Predicted=0, Result=0\n",
            "Predicted=0, Result=0\n",
            "Predicted=0, Result=0\n",
            "Predicted=1, Result=1\n",
            "Predicted=1, Result=1\n",
            "Predicted=1, Result=1\n",
            "Predicted=1, Result=1\n",
            "Predicted=1, Result=1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fk89B4XCiy6Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Comparisons\n",
        "\n",
        "## Regression?\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "92elhOEKU2uw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#\n",
        "# ID3 Decision Tree (Information Gain)\n",
        "#\n",
        "\n",
        "def find_entropy(df):\n",
        "    Class = df.keys()[-1]   #To make the code generic, changing target variable class name\n",
        "    entropy = 0\n",
        "    values = df[Class].unique()\n",
        "    for value in values:\n",
        "        fraction = df[Class].value_counts()[value]/len(df[Class])\n",
        "        entropy += -fraction*np.log2(fraction)\n",
        "    return entropy\n",
        "  \n",
        "  \n",
        "def find_entropy_attribute(df,attribute):\n",
        "  Class = df.keys()[-1]   #To make the code generic, changing target variable class name\n",
        "  target_variables = df[Class].unique()  #This gives all 'Yes' and 'No'\n",
        "  variables = df[attribute].unique()    #This gives different features in that attribute (like 'Hot','Cold' in Temperature)\n",
        "  entropy2 = 0\n",
        "  for variable in variables:\n",
        "      entropy = 0\n",
        "      for target_variable in target_variables:\n",
        "          num = len(df[attribute][df[attribute]==variable][df[Class] ==target_variable])\n",
        "          den = len(df[attribute][df[attribute]==variable])\n",
        "          fraction = num/(den+eps)\n",
        "          entropy += -fraction*log(fraction+eps)\n",
        "      fraction2 = den/len(df)\n",
        "      entropy2 += -fraction2*entropy\n",
        "  return abs(entropy2)\n",
        "\n",
        "\n",
        "def find_winner(df):\n",
        "    Entropy_att = []\n",
        "    IG = []\n",
        "    for key in df.keys()[:-1]:\n",
        "#         Entropy_att.append(find_entropy_attribute(df,key))\n",
        "        IG.append(find_entropy(df)-find_entropy_attribute(df,key))\n",
        "    return df.keys()[:-1][np.argmax(IG)]\n",
        "  \n",
        "  \n",
        "def get_subtable(df, node,value):\n",
        "  return df[df[node] == value].reset_index(drop=True)\n",
        "\n",
        "\n",
        "def buildTree(df,tree=None): \n",
        "    Class = df.keys()[-1]   #To make the code generic, changing target variable class name\n",
        "    \n",
        "    #Here we build our decision tree\n",
        "\n",
        "    #Get attribute with maximum information gain\n",
        "    node = find_winner(df)\n",
        "    \n",
        "    #Get distinct value of that attribute e.g Salary is node and Low,Med and High are values\n",
        "    attValue = np.unique(df[node])\n",
        "    \n",
        "    #Create an empty dictionary to create tree    \n",
        "    if tree is None:                    \n",
        "        tree={}\n",
        "        tree[node] = {}\n",
        "    \n",
        "   #We make loop to construct a tree by calling this function recursively. \n",
        "    #In this we check if the subset is pure and stops if it is pure. \n",
        "\n",
        "    for value in attValue:\n",
        "        \n",
        "        subtable = get_subtable(df,node,value)\n",
        "        clValue,counts = np.unique(subtable['Eat'],return_counts=True)                        \n",
        "        \n",
        "        if len(counts)==1:#Checking purity of subset\n",
        "            tree[node][value] = clValue[0]                                                    \n",
        "        else:        \n",
        "            tree[node][value] = buildTree(subtable) #Calling the function recursively \n",
        "                   \n",
        "    return tree"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T3wj_QJ8Yof3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def predict(inst,tree):\n",
        "    #This function is used to predict for any input variable \n",
        "    \n",
        "    #Recursively we go through the tree that we built earlier\n",
        "\n",
        "    for nodes in tree.keys():        \n",
        "        \n",
        "        value = inst[nodes]\n",
        "        tree = tree[nodes][value]\n",
        "        prediction = 0\n",
        "            \n",
        "        if type(tree) is dict:\n",
        "            prediction = predict(inst, tree)\n",
        "        else:\n",
        "            prediction = tree\n",
        "            break;                            \n",
        "        \n",
        "    return prediction"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5FC_67XPRsDu",
        "colab_type": "text"
      },
      "source": [
        "## Linear Regression Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jOTTZQ5kRwOu",
        "colab_type": "code",
        "outputId": "f7c70e94-1316-4c26-b029-6c4e66be8031",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "source": [
        "#import libraries\n",
        "%matplotlib inline\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "from math import sqrt\n",
        "\n",
        "#dataset = pd.read_csv('dataset.csv')\n",
        "dataset = [[2.771244718,1.784783929,0],\n",
        "\t[1.728571309,1.169761413,0],\n",
        "\t[3.678319846,2.81281357,0],\n",
        "\t[3.961043357,2.61995032,0],\n",
        "\t[2.999208922,2.209014212,0],\n",
        "\t[7.497545867,3.162953546,1],\n",
        "\t[9.00220326,3.339047188,1],\n",
        "\t[7.444542326,0.476683375,1],\n",
        "\t[10.12493903,3.234550982,1],\n",
        "\t[6.642287351,3.319983761,1]]\n",
        "\n",
        "dataset\n",
        "# print(dataset.shape)\n",
        "\n",
        "# dataset.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[2.771244718, 1.784783929, 0],\n",
              " [1.728571309, 1.169761413, 0],\n",
              " [3.678319846, 2.81281357, 0],\n",
              " [3.961043357, 2.61995032, 0],\n",
              " [2.999208922, 2.209014212, 0],\n",
              " [7.497545867, 3.162953546, 1],\n",
              " [9.00220326, 3.339047188, 1],\n",
              " [7.444542326, 0.476683375, 1],\n",
              " [10.12493903, 3.234550982, 1],\n",
              " [6.642287351, 3.319983761, 1]]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 75
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BwHC8gQnSaC6",
        "colab_type": "code",
        "outputId": "a37f329d-4b8a-4d44-b794-8d0c4123f69e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "# Define Calculation Functions\n",
        "\n",
        "# Calculate the mean value of a list of numbers\n",
        "def mean(values):\n",
        "\treturn sum(values) / float(len(values))\n",
        " \n",
        "# Calculate covariance between x and y\n",
        "def covariance(x, mean_x, y, mean_y):\n",
        "\tcovar = 0.0\n",
        "\tfor i in range(len(x)):\n",
        "\t\tcovar += (x[i] - mean_x) * (y[i] - mean_y)\n",
        "\treturn covar\n",
        " \n",
        "# Calculate the variance of a list of numbers\n",
        "def variance(values, mean):\n",
        "\treturn sum([(x-mean)**2 for x in values])\n",
        " \n",
        "# Calculate coefficients\n",
        "def coefficients(dataset):\n",
        "\tx = [row[0] for row in dataset]\n",
        "\ty = [row[1] for row in dataset]\n",
        "\tx_mean, y_mean = mean(x), mean(y)\n",
        "\tb1 = covariance(x, x_mean, y, y_mean) / variance(x, x_mean)\n",
        "\tb0 = y_mean - b1 * x_mean\n",
        "\treturn [b0, b1]\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1.9821291584841791, 1.8224808232595313, 2.1210154414361635, 2.1643044881025655, 2.017033765569457, 2.705794021992931, 2.936178767400693, 2.697678415506162, 3.108085804667108, 2.57484160958121]\n",
            "RMSE: 1.923\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZkCtdNoBbfT3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Calculate root mean squared error\n",
        "def rmse_metric(actual, predicted):\n",
        "\tsum_error = 0.0\n",
        "\tfor i in range(len(actual)):\n",
        "\t\tprediction_error = predicted[i] - actual[i]\n",
        "\t\tsum_error += (prediction_error ** 2)\n",
        "\tmean_error = sum_error / float(len(actual))\n",
        "\treturn sqrt(mean_error)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aG5qRCxybfDk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Evaluate regression algorithm on training dataset\n",
        "def evaluate_algorithm(dataset, algorithm):\n",
        "\ttest_set = list()\n",
        "\tfor row in dataset:\n",
        "\t\trow_copy = list(row)\n",
        "\t\trow_copy[-1] = None\n",
        "\t\ttest_set.append(row_copy)\n",
        "\tpredicted = algorithm(dataset, test_set)\n",
        "\tprint(predicted)\n",
        "\tactual = [row[-1] for row in dataset]\n",
        "\trmse = rmse_metric(actual, predicted)\n",
        "\treturn rmse"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MMQe1Qx8bfBU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Simple linear regression algorithm\n",
        "def simple_linear_regression(train, test):\n",
        "\tpredictions = list()\n",
        "\tb0, b1 = coefficients(train)\n",
        "\tfor row in test:\n",
        "\t\tyhat = b0 + b1 * row[0]\n",
        "\t\tpredictions.append(yhat)\n",
        "\treturn predictions\n",
        " \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VR1BZht7b6MG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Test simple linear regression\n",
        "rmse = evaluate_algorithm(dataset, simple_linear_regression)\n",
        "print('RMSE: %.3f' % (rmse))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ei-z2NGETlvE",
        "colab_type": "code",
        "outputId": "03e95c35-b84d-447e-b80d-0927dfa591b3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 533
        }
      },
      "source": [
        "# Plotting - needs work\n",
        "\n",
        "data = x,y\n",
        "colors = (\"red\", \"blue\")\n",
        "groups = (\"X1\", \"X2\")\n",
        "\n",
        "# Create plot\n",
        "fig = plt.figure()\n",
        "ax = fig.add_subplot(1, 1, 1)\n",
        "\n",
        "for data, color, group in zip(data, colors, groups):\n",
        "  x = data\n",
        "  y = data\n",
        "  ax.scatter(x, y, alpha=0.8, c=color, edgecolors='none', s=30, label=group)\n",
        "\n",
        "plt.title('Scatter plot')\n",
        "plt.legend(loc=2)\n",
        "plt.show()\n",
        "\n",
        "\n",
        "plt.plot(x, y)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXsAAAEICAYAAAC+iFRkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nZmYZ8P8BcSJmzRUaIwQAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nnYjIgVjuREQOxHInInKg/weLFUSVpGWQLgAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ge1JCa9NbJs8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Show comparison with a Large/small dataset\n",
        "\n",
        "## Show comparison of predictive accuracy between Decision Tree & Linear Regression"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ISzhqah26-KQ",
        "colab_type": "text"
      },
      "source": [
        "##Evaluation (200)\n",
        "\n",
        "\n",
        "**Report Execution on Data**\n",
        "\n",
        "**Report Testing**\n",
        "\n",
        "**Efficiency Analysis**\n",
        "\n",
        "**Comparative Study**\n",
        "\n",
        "ideas:\n",
        "* how does each feature distribution look like? Are there any differences between feature distribution in train and test data?\n",
        "* are there any meaningful interactions between the features?\n",
        "* are there outliers and can they be explained?\n",
        "* are there missing values or duplicates? What are reasons for them?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G-FTRF_s85K_",
        "colab_type": "text"
      },
      "source": [
        "## Conclusion (100)\n",
        "\n",
        "\n",
        "Discuss Reflections\n",
        "\n",
        "Propose Possible Improvements\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gLV7jwde8-oX",
        "colab_type": "text"
      },
      "source": [
        "## Ethical (200)\n",
        "\n",
        "* Discuss the social/ethical aspect of the project\n",
        "  * adopt an ethical model (e.g. Ultitarian or Kantian)\n",
        "* Consider how the technique could be misused"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TdspPnHH9Cn3",
        "colab_type": "text"
      },
      "source": [
        "## Video Pitch\n",
        "\n",
        "[url to video]\n",
        "\n",
        "**Highlight Challenges and Effort**\n",
        "\n",
        "* Describe challenges and how the team addressed them."
      ]
    }
  ]
}