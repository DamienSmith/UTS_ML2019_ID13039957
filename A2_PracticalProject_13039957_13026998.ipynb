{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of Untitled2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DamienSmith/UTS_ML2019_ID13039957/blob/master/A2_PracticalProject_13039957_13026998.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TWf4FTWL7mhY",
        "colab_type": "text"
      },
      "source": [
        "# A2: Algorithm Implementation | 31005 | Advanced Data Analytics\n",
        "Student: 13039957 & 13026998\n",
        "\n",
        "Link to Github:\n",
        "https://github.com/DamienSmith/UTS_ML2019_ID13039957/blob/master/A2_PracticalProject_13039957_13026998.ipynb\n",
        "\n",
        "Video url: https://youtu.be/mEefphtMhcc"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N0-ciK386-52",
        "colab_type": "text"
      },
      "source": [
        "##Introduction - a project overview\n",
        "\n",
        "\n",
        "**The Decision Tree (CART) Algorithm** \n",
        "\n",
        "A decision tree is a decision support tool that models possible outcomes based on a number of conditions (Singh & Gupta 2014). Starting with a root node, an algorithm is applied that splits into additional nodes recursively resulting in a workflow that predicts an outcome (Peng, Chen & Zhou 2009). Common decision tree algorithms include Iterative Dichotomiser (ID3) and Classification and Regression Trees (CART) which use different splitting criteria to determine a score that identifies the best way to split a dataset (Singh & Gupta 2014).\n",
        "\n",
        "For this project, the CART algorithm was implemented from scratch using Python 3. The representation of the CART model is a binary tree where each node can have zero, one or two child nodes (Brownlee 2016). All input variables and all possible split points are evaluated and chosen in a greedy manner based on the Gini impurity function. The Gini impurity provides a purity score representing the miss-classification rate within a dataset. A split is made on the best score at each level of depth. Splitting continues until a stopping condition is reached such as nodes contain a minimum number of training examples or a maximum tree depth is reached. Once created, a tree can be navigated by following the rules at each branch until a final prediction is reached.\n",
        "\n",
        "CART has the following advantages and disadvantages (Timofeev 2004) & (Singh & Gupta 2014):\n",
        "\n",
        "Advantages:\n",
        "* CART can handle both numerical and categorical variables.\n",
        "* CART algorithm will identify the most significant variables.\n",
        "* CART can handle outliers.\n",
        "\n",
        "Disadvantages:\n",
        "* CART may have unstable decision tree. Minor modification such as changes in splitting variables and values can greatly increase or decrease tree complexity. \n",
        "* CART can only split by one variable.\n",
        "\n",
        "\n",
        "**Define Input/Output** \n",
        "\n",
        "This algorithm was designed to handle any dataset that includes features and target columns with numeric values. The target column needs to contain only one value (i.e. not a multivariable entry). For this reason, we have imported the sklearn dataset library to provide the user input to experiment with that meet these requirements.  Note, that the dataset is turned into a Panda dataframe. Pandas is a Python library providing integrated, intuitive routines for performing common data manipulations and analysis on data sets (McKinney 2011). The provided code will convert the sklearn database into a pandas dataframe and build a decision tree based on the maxdepth inputted by the users. After running the algorithm, the decision tree and the predicted value of each tested instance will be printed in the command line.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-076P8Yx6-iu",
        "colab_type": "text"
      },
      "source": [
        "##Exploration\n",
        "\n",
        "**Practical Significance**\n",
        "\n",
        "Decision trees are practically significant as they are intuitively understood by laypeople such as stakeholders. This makes it a useful tool for explanation purposes. They also form a foundational part of understanding machine learning. \n",
        "\n",
        "The implementation of the CART decision tree algorithm is a practically significant project because it represents the value of decision tree learning. These include:\n",
        "\n",
        "1.   Demonstrating the generalization of unobserved instances (when described in terms of features that are correlated with a target class).\n",
        "2. Demonstrating that the computation efficiency is proportional to the number of observed training instances (the size of the dataset).\n",
        "3.   Demonstrating that a decision tree model as an intuitive classification process that is relatively easy to understand (e.g. doctors use it when diagnosing patients).\n",
        "\n",
        "\n",
        "**Challenges**\n",
        "\n",
        "The following challenges were encountered when designing our decision tree, and implementing the solution:\n",
        "\n",
        "* Dataset Selection\n",
        "\n",
        "A challenge faced was having an appropriate dataset to use for training the CART decision tree. This was solved by using sklearn database datasets that meet the requirements for implementing the algorithm. As is expected, this project cannot handle multivariable data as this is a known limitation of CART.\n",
        "\n",
        "* Finding the best split\n",
        "\n",
        "Originally the net sum of the gini impurity at each split point was calculated by summing the gini impurity of both the left and right side of each possible split. We found that although it performed well for some splits,  it would start to favour ‘edge’ cases; i.e. choosing a split close to an end point since that would give the best ‘net’ impurity. This was solved by implementing a weighted net gini impurity. This was calculated based on the proportion of data points remaining in each split point/leaf. This weighted calculation gave less importance to edge cases and improved the quality of splitting.\n",
        "\n",
        "* Training Storage\n",
        "\n",
        "Another challenge faced was how to ‘store’ the structure of the decision tree. It was easy to identify how to do each split and draw the tree manually, however, putting that information into Python proved difficult. \n",
        "\n",
        "The first approach was a ‘test’ row would be stored, and then tested simultaneously as the tree was created. This would have been disastrous for performance, however, since every entry would have a new tree created. \n",
        "\n",
        "We solved this by storing the tree model into a Python dictionary, storing every possible node value. For example at depth 1, we created a dictionary containing 2 sets of nodes with all possible outcomes. For depth 2, we created 4, and for depth 3 we’d create 8. This proved effective as we are able to see a ‘complete’ and symmetrical tree,  however it is also exponentially taxing, computationally as the depth increases.\n",
        "\n",
        "* Predicting & Pruning\n",
        "\n",
        "Knowing when the decision tree should ‘commit’ to a prediction was a challenge. It was decided that the algorithm should make a ‘decision’ if there was only one target value remaining for a particular leaf. For example, with the Iris data set, splitting at Petal Length <= 1.9, resulted in only 0 or ‘Setosa’ Irises left. Further improving this, a decision was set to be made when the max depth parameter was met. Note that if more than one target value remained on a leaf node the target value with the highest mode, and if the mode was equal, the first entry would be selected as the decision. \n",
        "\n",
        "This might still lead to challenges if depth is increased too much, causing overfitting to become more prominent since it may split when only one target data point remains. \n",
        "Pruning also provided a challenge as there is no way to check when to stop pruning. The algorithm required manual observation to determine the best depth to ‘prune’ the tree. This project kept to a ‘max depth’ method that requires ‘trial and error’ in order to check each depth for meaningful predictions.. \n",
        "\n",
        "\n",
        "\n",
        "**Design Data Structures**\n",
        "\n",
        "In general, a learning problem considers a set of n samples of data and then tries to predict properties of unknown data. When planning the algorithm implementation we started with the maths that defined how to calculate the best split points (Hastie et al 2008, at p 308). \n",
        "\n",
        "It was decided to use the gini impurity to determine impurity since we had a classification (as opposed to a regression) problem, and gini impurity was the more appropriate measure of impurity as it was differentiable, sensitive to change and was intuitively understood (Hastie et al 2008, at p 310). \n",
        "\n",
        "We also decided to do binary splits rather than multiway splits as we didn’t want to split the data too quickly, especially since datasets like the Iris set did not have a huge amount of data to split. \n",
        "\n",
        "When determining the decision node, we decided that unique value as a deciding factor mixed with a maximum depth would be appropriate, especially upon a visual inspection of the Iris data set. Other options such as a minimum impurity level or working out smoothness or linear combination splits seemed unnecessarily complex for our given data. \n",
        "\n",
        "\n",
        "* data acquisition\n",
        "\n",
        "The Scikit-learn library provides datasets, for example the iris and wine datasets for classification and the boston house prices dataset for regression (Louppe 2016). Each dataset is a Python dictionary-like object that holds all the data and some metadata about the data. This data is stored in the data member, which is a n_samples and n_features array.\n",
        "\n",
        "In the case of supervised problem, one or more response variables are stored in the target member. This is the number corresponding classification that we are trying to learn.\n",
        "For each dataset, the.data member gives access to the features that can be used to classify the digits\n",
        "\n",
        "* quality control\n",
        "\n",
        "Quality control refers to the state of completeness, conformity, consistency, timeliness, duplication, integrity, and accuracy that makes data appropriate for a specific use (Strong, Lee. & Wang 1997). For the purpose of simplification, the sklearn database library has been used as a way to manage the data quality of each dataset. Each dataset  is provided in identical format, which has allowed the focus of the project to be on the algorithm implementation and streamline a common standard of data input.\n",
        "\n",
        "* modelling techniques\n",
        "\n",
        "The decision tree was modeled into a series of python dictionaries, which represented nodes of a tree. Each dictionary holds relevant information of the depth, decision, feature, value and right/left value for each node. Each level of depth was used as a reference and further rules applied if ‘decision’ was none. The right left value is used to determine if the value is greater than or less than.\n",
        "\n",
        "An example of a python dictionary decision tree model on the iris dataset after running subsequent_split(df, 1, 3)\n",
        "\n",
        "      {1: {'decision': 0,\n",
        "\n",
        "           'feature': 'petal length (cm)',\n",
        "           'side': 'left',\n",
        "           'value': 1.9},\n",
        "\n",
        "       2: {'decision': None,\n",
        "           'feature': 'petal length (cm)',\n",
        "           'side': 'right',\n",
        "           'value': 1.9}}\n",
        "\n",
        "      {1: {0},\n",
        "\n",
        "        2: {0},\n",
        "\n",
        "       3: {'decision': 1,\n",
        "\n",
        "           'feature': 'petal width (cm)',\n",
        "           'side': 'left',\n",
        "           'value': 1.7},\n",
        "\n",
        "       4: {'decision': 1,\n",
        "           'feature': 'petal width (cm)',\n",
        "           'side': 'right',\n",
        "           'value': 1.7}}\n",
        "\n",
        "      {}\n",
        "\n",
        "CART is characterized by its construction of binary trees where each internal node has exactly two outgoing edges (Breiman et al. 1984). The splits for our implementation are selected by finding the lowest weighted net impurity score on features as the splitting criteria. The prediction in each leaf is based on the weighted mode for a node.\n",
        "\n",
        "* Split Calculations:\n",
        "\n",
        "* Gini Impurity: \n",
        "\n",
        "${\\displaystyle 1 – \\sum _{i \\neq j}(P_{i}P_{j})^2}$\n",
        "\n",
        "* Weighted net impurity: \n",
        "\n",
        "sum of the impurity of the left and right leaves weighted by the proportion of data points on the respective side. \n",
        "\n",
        "* Evaluation Method: \n",
        "\n",
        "If predicted value is equal to the target value than it is correct.\n",
        "The details of its implementation is covered in the methodology section of this report.\n",
        "\n",
        "The details of its implementation is covered in the methodology section of this report.\n",
        "\n",
        "**Plan Data Models and Tests**\n",
        "\n",
        "The project is designed to work in Google Colaboratory (Colab). Colab is a cloud service based on Jupyter Notebooks for disseminating machine learning education and research (Carneiro et al 2018). The comments in the code guide the user through the code logic.\n",
        "\n",
        "As a user of the code, you are invited to input different sklearn databases through the dropdown menu provided. A user can also enter the maximum depth of a tree by inputting it into the subsequent_split() function. After building a model, a user can input test data that has automatically been split off the input dataset. Running the testDataAccuracy() function will test the performance of the model and provide an overall accuracy score.\n",
        "\n",
        "\n",
        "**Possible alternatives**\n",
        "\n",
        "A number of possible alternatives presented themselves while researching decision tree implementation, these include:\n",
        "\n",
        "* **Impurity functions:** Use additional impurity functions such as cost function/entropy to see which is the most appropriate impurity function for a given data set.\n",
        "\n",
        "* **Tree Pruning.** An important technique for reducing overfitting of the training dataset is to prune the trees.  The algorithm currently determines a maximum tree depth manually, however with methods such as grid searching, a better method to determine the optimal tree depth could be made.\n",
        "\n",
        "* **Terminal nodes.** Alternative methods for splitting the data could be made such as minimum impurity levels, having a minimum observation requirement for a terminal node or maximum number of possible nodes.\n",
        "\n",
        "* **Algorithm Tuning:** experimenting with different parameter to achieve better performance, such as weighing a minimum classes (if the dataset was not balanced)\n",
        "\n",
        "\n",
        "* **Categorical Dataset.** The example was designed for input data with numerical or ordinal input attributes. Whilst a data set should optimally be cleaned to only contain numerical data, the algorithm could be expanded to be more flexible with messy data such as dealing with categorical input data or missing data.\n",
        "\n",
        "\n",
        "* **Different datasets.** The algorithm fits relatively well with the Iris dataset, however alternatives could be done to include more datasets\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dv3kz8Cx6-bm",
        "colab_type": "text"
      },
      "source": [
        "##Methodology\n",
        "\n",
        "\n",
        "The general method we undertook when creating the devision tree algorithm was as follows:\n",
        "\n",
        "\n",
        "1.   Input dataset (sklearn database):\n",
        "\n",
        "  1.1   Ensure that the dataset was cleaned and contains non-categorical data; and\n",
        "  \n",
        "  1.2  Split into training/testing dataset.\n",
        "\n",
        "\n",
        "2.   For every feature in the training dataset:\n",
        "\n",
        "  2.1 Select each value for each feature in the dataset;\n",
        "  \n",
        "  2.2 Calculate gini index for the left (less than and equal to) and right (greater than) side of each of these values for that feature;\n",
        "  \n",
        "  2.3 Calculate the ‘weight’ of number of targets for each side of split; \n",
        "  \n",
        "  2.4 Pick the best weighted net gini score (lowest score);\n",
        "  \n",
        "  2.5 Split dataset and start the next ‘depth’ layer; and \n",
        "  \n",
        "  2.6 storing the node/split decision in a dictionary.\n",
        "\n",
        "3. Loop this feature selection and splitting until:\n",
        "\n",
        "    3.1 Max depth reached; or \n",
        " \n",
        "    3.2 Only unique target values remain in split dataset; otherwise\n",
        "\n",
        "    3.3 Continue splitting.\n",
        "\n",
        "4. When predicting, for every row in testing dataset\n",
        "\n",
        "\n",
        "    4.1 Test features against decision tree dictionary for each relevant depth\n",
        "    4.2 Print row and predicted value\n",
        "    4.3 Print overall prediction accuracy\n",
        "\n",
        "\n",
        "Additional explanation can be found in the comments\n",
        " \n",
        "Gini Impurity calculation:\n",
        "\n",
        "*   for each class:\n",
        "  * Calculate probability of class in the given branch.\n",
        "  * Square the class probability.\n",
        "*   Sum the squared class probabilities\n",
        "  * Subtract the sum from 1.\n",
        "  * Weight each branch based on the baseline probability.\n",
        "  * Sum the weighted gini index for each split.\n",
        "\n",
        "A perfectly classified, Gini Index would be zero. An evenly distributed would be 1 – (1/# Classes). You want a variable split that has a low Gini Index.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JuFQehMoR0uz",
        "colab_type": "text"
      },
      "source": [
        "## Decision Tree Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P9F55nV5LWxV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## \n",
        "##    A2: Algorithm Implementation | 31005 | Advanced Data Analytics\n",
        "##    \n",
        "##    Authors: Rae Ho (13026998) & Damien Smith (13039957)\n",
        "##    Goals: - Implement Decision Tree Algorithm\n",
        "##           - Build & Train a model\n",
        "##           - Explore/Compare different parameters \n",
        "##    Code: Python 3\n",
        "##    Github: https://github.com/DamienSmith/UTS_ML2019_ID13039957/blob/master/A2_PracticalProject_13039957_13026998.ipynb\n",
        "##\n",
        "##"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ocOEBp3jxjcY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Import Libraries\n",
        "#numpy to work with arrays\n",
        "#panda for dataframe \n",
        "#sklearn to import iris dataset\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EOIwEKy9xmGh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Get data\n",
        "#using iris data. Get it from sklearn dataset\n",
        "dataset = datasets.load_iris() #@param [\"datasets.load_iris()\", \"datasets.load_boston()\", \"datasets.load_diabetes()\" , \"datasets.load_wine()\", \"datasets.load_breast_cancer()\"]{type:\"raw\"}\n",
        "\n",
        "# We want to turn the dataset into a panda dataframe\n",
        "df = pd.DataFrame(dataset['data'])   \n",
        "\n",
        "\n",
        "# We also want to add the names of each feature to the dataframe\n",
        "df.columns = [name[:] for name in dataset['feature_names']]\n",
        "\n",
        "#We want to add the target to the dataframe too \n",
        "df['target'] = dataset['target']\n",
        "\n",
        "#SORRY COLLAB WASN'T COMPILING PROPERLY LAST NIGHT -->\n",
        "#X_train, X_test, y_train, y_test = train_test_split(df, df['target'], test_size=0.3, random_state=100)\n",
        "#for simplicity in showing target variables for accuracy we will be 'overfitting' by taking the test data\n",
        "#as a subset of the training data. Normally this should not be done and the above split method should be used\n",
        "\n",
        "#this is to create a random sample of 30 entries\n",
        "testdf = df.sample(n=30, random_state=1).reset_index(drop=True) \n",
        "\n",
        "\n",
        "# We create target variable (into binary) in order to get a 'count' of all items in the dataframe (in case there are missing values) \n",
        "df['dummy'] = 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jp8GOuVhxrB6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#function to return the count of rows in the dataframe that match a condition\n",
        "\n",
        "def count_target(dframe, right, feature, value):\n",
        "  #this is to find the impurity to the left of and to the right of a slice on a cartesian plane   \n",
        "  if right:\n",
        "    cond = dframe[feature]>value\n",
        "  else:\n",
        "    cond = dframe[feature]<=value\n",
        "  \n",
        "  #this works by counting the total number of target items, 'mean' helps find probability of correct choice \n",
        "  #count = len(dframe[cond])\n",
        "  count = sum(cond)\n",
        "  return count"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AG-qabusxsxg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#We want to create a method to determine what the impurity is for any 'slice' of a feature. \n",
        "#We have decided to use the gini impurity which shows how likely it is that a randomly selected feature is correctly guessed\n",
        "\n",
        "##this takes the parameters:\n",
        "    #dframe which allows you to put in the dataframe you want to find the impurity for\n",
        "    #right which takes True or False to look at right or left impurity of a split point\n",
        "    #feature which is the feature (column) that you want to use/look at\n",
        "    #value which is the value that is to be split at\n",
        "    #target variable to be determined (which for the iris df we are using is 'target')\n",
        "\n",
        "\n",
        "#find gini impurity of a slice \n",
        "def gini_impurity_leaf(dframe, right, feature, value, target_variable):\n",
        "\n",
        "#this is to find the impurity to the left of and to the right of a slice on a cartesian plane   \n",
        "  if right:\n",
        "    cond = dframe[feature]>value\n",
        "  else:\n",
        "    cond = dframe[feature]<=value\n",
        "  \n",
        "  #this works by counting the total number of target items, 'mean' helps find probability of correct choice \n",
        "  count_of_target = dframe[cond].groupby(target_variable)['dummy'].count()\n",
        "  #print(\"count of target is: \")\n",
        "  #print(count_of_target)\n",
        "  #this uses the impurity formula of impurity = 1 - sum of (probability of being correct [count/length])^2\n",
        "  gini_impurity = 1 - (np.divide(count_of_target, len(dframe[cond])) ** 2).sum()\n",
        "  \n",
        "  return gini_impurity"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xuFwUzgBxxf2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#this finds the net gini impurity by adding the left and right slice\n",
        "# we want to find the 'slice' with the lowest net gini impurity \n",
        "\n",
        "def net_gini_impurity(dframe, feature, value, target_variable):  \n",
        "\n",
        "#  net_gini = gini_impurity_leaf(dframe, True, feature, value, target_variable) + gini_impurity_leaf(dframe, False, feature, value, target_variable) #this didn't work as it would have duplicate values or prefer edge cases\n",
        "# we had to use a weighted gini index\n",
        "\n",
        "  right_weight = count_target(dframe, True, feature, value) / len(dframe)\n",
        "  left_weight = 1 - right_weight\n",
        " #print(\"right weight is:{}\".format(right_weight))\n",
        "  net_gini = (right_weight * gini_impurity_leaf(dframe, True, feature, value, target_variable)) + (left_weight * gini_impurity_leaf(dframe, False, feature, value, target_variable))\n",
        "  \n",
        "  return net_gini"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KHx3uOwHxzHZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# This finds best split \n",
        "\n",
        "def find_best_split(dataframe):\n",
        "# Find the best split by going over every value in each feature and choosing the split with the lowest net impurity\n",
        "    lowest_impurity = 2  # keep track of the worst impurity\n",
        "    best_split = ['there is no split', 0]  # keep train of the feature / value that produced it\n",
        "\n",
        "    \n",
        "    for f in range(len(dataframe.columns) -2):  # loop through each feature --> the -2 is because it shouldnt look at TARGET and DUMMY\n",
        "        feat = dataframe.columns[f]             # store the name of the feature as 'feat'\n",
        "        for val in dataframe[feat].unique():  # loop through each unique value for that feature \n",
        "          #Calculate the net impurity of each value in feature\n",
        "          #print(feat,val)\n",
        "          net_imp = net_gini_impurity(dataframe, feat, val, 'target')\n",
        "          #print(feat,val,net_imp)\n",
        "          split = [feat, val]\n",
        "          # store best gain and best feature \n",
        "          if net_imp < lowest_impurity:\n",
        "            lowest_impurity = net_imp\n",
        "            best_split = split\n",
        "            #print(\"the current best split is:\")\n",
        "            #print(lowest_impurity,best_split)\n",
        "\n",
        "    return lowest_impurity, best_split"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "unGp9Ylcx1Cn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#this splits a dataframe to right and left sides\n",
        "\n",
        "def splitter(dframe):\n",
        "  impurity, featsplit = find_best_split(dframe)\n",
        "  feature = featsplit[0]\n",
        "  value = featsplit[1]\n",
        "  \n",
        "  #and store the values of those sides into a new data_frame  \n",
        "  right_split = dframe[dframe[feature]>value]\n",
        "  left_split =dframe[dframe[feature]<=value]      \n",
        "  \n",
        "  #print(\"I have been split for the feature \", feature, \"at value\", value)\n",
        "  \n",
        "  return right_split, left_split, feature, value"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Anzk0Dzlx3W-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#This returns what the target is\n",
        "# This returns a predicted value for the terminal node\n",
        "# The returned value is the end value with the most common end result\n",
        "\n",
        "#def final_guess(feature, value, dframe, boolean=False):\n",
        "def final_guess(dframe):\n",
        "\n",
        "  dataset = dframe\n",
        "  targets = dataset.target\n",
        "  outcome = None\n",
        "  \n",
        "  outcome = targets.value_counts().idxmax()  #this is the first most common value in that set - so if it's a 50/50 split between two targets, it'll choose the first one.\n",
        "  \n",
        "  return outcome\n",
        "\n",
        "  #alternatively you could do 'outcomes = targets.mode()' but then you wont get a single int if there's multiple values"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lBvGZ4pwx_VX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#this creates an depth of arrays which we use to store values for later predict\n",
        "#the aim is to create a list of dictionaries to store the trained data\n",
        "\n",
        "def createDepthArray(maxdepth):\n",
        "  for i in range(1,maxdepth):\n",
        "    depth = i\n",
        "    name = 'depth{}'.format(depth)          #get the name of the dict array\n",
        "\n",
        "    #populate the array with futher arrays\n",
        "    if name not in globals():       \n",
        "      globals()['depth%s' % depth] = {}       #create an array called depth#    \n",
        "    \n",
        "    if len(globals()['depth%s' % depth]) < 1:\n",
        "      for n in range(1, 2**depth + 1):              \n",
        "        globals()['depth%s' % depth][n] = {}\n",
        "      #elif name in globals():\n",
        "      #  print(\"already exists\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GMyXfP-pyE7R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#this helps insert values into the previous 'depth array' parts\n",
        "def insertNodeValue(depth, number, feature, value, side, guess):     \n",
        "  #number = getNumberPos(depth)\n",
        "  globals()['depth%s' % depth][number] = {'feature': feature, 'value': value, 'side': side, 'decision': guess}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i8vbZDLuyLvH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#to fill subsequent depths's numbers that already has a final decision   \n",
        "\n",
        "def depthFiller(depth, outcome, number, maxdepth):\n",
        "  new_depth = depth+1\n",
        "  #number = getNumberPos(depth)\n",
        "  if new_depth < maxdepth:\n",
        "\n",
        "    #print(\"i am at position {} and am filling lower depth\".format(number))\n",
        "    number_left = (number*2 - 1)\n",
        "    number_right = (number * 2)\n",
        "    globals()['depth%s' % new_depth][number_right] = {outcome}\n",
        "    globals()['depth%s' % new_depth][number_left] = {outcome}\n",
        "    #and then repopulate the sub-depths until max-depth\n",
        "    depthFiller(new_depth, outcome, number_right, maxdepth)\n",
        "    depthFiller(new_depth, outcome, number_left, maxdepth)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wSYczhFayORy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#this returns the next empty number for that depth\n",
        "def getNumberPos(depth):\n",
        "  for i in range(1, 2**depth+1):\n",
        "    if len(globals()['depth%s' % depth][i]) == 0:\n",
        "      return i\n",
        "      break\n",
        "    else:\n",
        "      continue"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7xrstiyZyRVE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#this is the main code that creates the tree by iteratively splitting\n",
        "#data at the best point\n",
        "\n",
        "def subsequent_split(dataframe, depth, maxdepth):\n",
        "  global maxDepth\n",
        "  maxDepth = maxdepth\n",
        "  createDepthArray(maxdepth)\n",
        "  depthspace = \"  \"*depth\n",
        "  right_split, left_split, feature, value = splitter(dataframe)\n",
        "  #create a bunch of dict to max depth\n",
        "  \n",
        "\n",
        "  # store:  [number: {feature = '', value ='', side = 'left', decision = 0,1,3,null}]\n",
        "\n",
        "  if depth < maxdepth:\n",
        "    #print(\"I am doing left side\")\n",
        "    \n",
        "    if left_split.empty:\n",
        "      print(\"I am working at depth\", depth)\n",
        "      print(\"left side is empty\")\n",
        "      \n",
        "    #this currently looks for a unique value, and if only 1 \n",
        "    #consider doing a mininum impurity    \n",
        "    elif len(left_split['target'].unique()) < 2:\n",
        "      print(\"{} I am working at depth {}\".format(depthspace, depth))\n",
        "      #print(\"i am now uniquely left\")\n",
        "      guess = final_guess(left_split)\n",
        "      print(\"{} for {} <= {}, we predict {}\".format(depthspace, feature, value, guess))\n",
        "      number = getNumberPos(depth)\n",
        "      depthFiller(depth, guess, number, maxdepth)\n",
        "      insertNodeValue(depth, number, feature, value, 'left', guess)\n",
        "\n",
        "    #if we have reached max depth, then it will predict anyway\n",
        "    elif depth == maxdepth - 1:\n",
        "      guess = final_guess(left_split)\n",
        "      print(\"{} for {} <= {}, we predict {}\".format(depthspace, feature, value, guess))\n",
        "      number = getNumberPos(depth)\n",
        "      depthFiller(depth, guess, number, maxdepth)\n",
        "      insertNodeValue(depth, number, feature, value, 'left', guess)\n",
        "    \n",
        "    #else if it hasnt reached max depth, then it will keep splitting\n",
        "    else:  \n",
        "      #print(len(left_split['target'].unique()))\n",
        "      print(\"{} I am working at depth {}\".format(depthspace, depth))\n",
        "      print(\"{} i am now branching for {} <= {}\".format(depthspace, feature, value))\n",
        "      number = getNumberPos(depth)\n",
        "      insertNodeValue(depth, number, feature, value, 'left', None)\n",
        "      subsequent_split(left_split,depth+1,maxdepth)\n",
        "     \n",
        "    #print(\"I am doing right side\")\n",
        "    if right_split.empty:\n",
        "      #print(\"depth is\", depth)\n",
        "      print(\"right side is empty\")  \n",
        "    \n",
        "    #checks if theres a unique target left\n",
        "    elif len(right_split['target'].unique()) < 2:  \n",
        "      print(\"{} I am working at depth {}\".format(depthspace, depth))\n",
        "      #print(\"i am now uniquely right\")\n",
        "      guess = final_guess(right_split)\n",
        "      print(\"{} for {} > {}, we predict {}\".format(depthspace, feature, value, guess))\n",
        "      number = getNumberPos(depth)\n",
        "      depthFiller(depth, guess, number, maxdepth)\n",
        "      insertNodeValue(depth, number, feature, value, 'right', guess)\n",
        "    \n",
        "    #checks if max depth is reached and if so predict\n",
        "    elif depth == maxdepth - 1:\n",
        "      guess = final_guess(left_split)\n",
        "      print(\"{} for {} > {}, we predict {}\".format(depthspace, feature, value, guess))\n",
        "      number = getNumberPos(depth)\n",
        "      depthFiller(depth, guess, number, maxdepth)\n",
        "      insertNodeValue(depth, number, feature, value, 'right', guess)\n",
        "    \n",
        "    #else keep splitting\n",
        "    else:\n",
        "      #print(\"depth is\", depth)\n",
        "      print(\"{} I am now branching for {} > {}\".format(depthspace, feature, value))\n",
        "      number = getNumberPos(depth)\n",
        "      insertNodeValue(depth, number, feature, value, 'right', None)\n",
        "      subsequent_split(right_split,depth+1,maxdepth)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GI7Zbsh9yah6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#def to clear the dictionary. Needed if we want to re-train/subsequent_split  \n",
        "def clearTrainedData():\n",
        "  for n in range(1, maxDepth):              \n",
        "    globals()['depth%s' % n].clear()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f1u4v5VnyfOq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#this is for subsequent predictions after the first depth\n",
        "def subsequentPredict(test_df_row, number, depth):\n",
        "  index = number\n",
        "  depth = depth + 1\n",
        "  if depth < maxDepth:\n",
        "    for x in test_df_row.columns:               #for each df header            \n",
        "      if globals()['depth%s' % depth][index]['feature'] == x:           #to see if feature is same\n",
        "        dictVal = globals()['depth%s' % depth][index]['value']                # value of depth1      \n",
        "        for n in range(len(test_df_row[x])):     #loop each index row in testdf        \n",
        "          rowVal = test_df_row.loc[n, x ]   #store the value for that row\n",
        "      \n",
        "          if globals()['depth%s' % depth][index]['side'] == 'right':      # if depth entry is left or right\n",
        "            if rowVal > dictVal:\n",
        "              if globals()['depth%s' % depth][index]['decision'] is not None:\n",
        "                print(\"the prediction is {}\".format(globals()['depth%s' % depth][index]['decision']))\n",
        "              else:\n",
        "                #print(\"need to continue\")\n",
        "                subsequentPredict(test_df_row, index*2 - 1, depth)\n",
        "                subsequentPredict(test_df_row, index*2, depth)\n",
        "                \n",
        "          if globals()['depth%s' % depth][index]['side'] == 'left':      # if depth entry is left or right   \n",
        "            if rowVal <= dictVal:\n",
        "              if globals()['depth%s' % depth][index]['decision'] is not None:\n",
        "                print(\"the prediction is {}\".format(globals()['depth%s' % depth][index]['decision']))\n",
        "              else:\n",
        "                #print(\"need to continue\")\n",
        "                subsequentPredict(test_df_row, index*2 - 1, depth)\n",
        "                subsequentPredict(test_df_row, index*2, depth)\n",
        "  #else:\n",
        "  #  print(\"reached max depth already\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GR8PJ2OOyj1K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def firstPredict(test_df_row):\n",
        "  for x in test_df_row.columns:               #for each df header\n",
        "    for i in range(1,len(depth1)+1):          #loop through each dict key\n",
        "      if depth1[i]['feature'] == x:           #to see if feature is same\n",
        "        #print(\"feature to look over is{}\".format(x))\n",
        "        dictVal = depth1[i]['value']                # value of depth1\n",
        "        #print(\"dictval is {}\".format(dictVal))\n",
        "        #print(type(dictVal))\n",
        "        \n",
        "        for n in range(len(test_df_row[x])):     #loop each index row in testdf        \n",
        "          rowVal = test_df_row.loc[n, x ]   #store the value for that row\n",
        "          #print(\"rowVal is {}\".format(rowVal))\n",
        "          #print(type(rowVal))\n",
        "          #print(depth1[i]['side'])\n",
        "          \n",
        "          if depth1[i]['side'] == 'right':      # if depth1 entry is left or right\n",
        "            if rowVal > dictVal:\n",
        "              #print(\"Row Val is greater than DictVal\")\n",
        "              if depth1[i]['decision'] is not None:\n",
        "                print(\"the prediction is {}\".format(depth1[i]['decision']))\n",
        "              elif depth1[i]['decision'] is None:\n",
        "                #print(\"need to continue\")\n",
        "                subsequentPredict(test_df_row, i*2 - 1, 1)\n",
        "                subsequentPredict(test_df_row, i*2, 1)\n",
        "            #else:\n",
        "            #  print(\"rowVal does not match this node\")\n",
        "          \n",
        "          if depth1[i]['side'] == 'left':      # if depth1 entry is left or right   \n",
        "            if rowVal <= dictVal:\n",
        "              #print(\"Row Val is less than DictVal\")\n",
        "              if depth1[i]['decision'] is not None:\n",
        "                print(\"the prediction is {}\".format(depth1[i]['decision']))\n",
        "              elif depth1[i]['decision'] is None:\n",
        "                #print(\"need to continue\")\n",
        "                subsequentPredict(test_df_row, i*2 - 1, 1)\n",
        "                subsequentPredict(test_df_row, i*2, 1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PtZSHKf3ypOP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#this is to test each row of the inputed dataframe\n",
        "def testData(dataframe):\n",
        "  testdata = dataframe\n",
        "  \n",
        "  for i in range(len(testdf)):\n",
        "    testrow = testdata[:1]\n",
        "    print(\"for the row:\")\n",
        "    print(testrow.to_string(index=False))\n",
        "    firstPredict(testrow)\n",
        "    testdata = testdata.drop([0,0]).reset_index(drop=True)\n",
        "    print(\"\\n\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wygVTTe09UmO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#this tests each row but can work out accuracy when the target value is known \n",
        "def testDataAccuracy(dataframe):\n",
        "  from io import StringIO  # Python3\n",
        "  import sys\n",
        "  \n",
        "  testdata = dataframe\n",
        "  count = 0\n",
        "  correct = 0\n",
        "  for i in range(len(testdf)):\n",
        "    testrow = testdata[:1]\n",
        "    print(\"for the row:\")\n",
        "    print(testrow.to_string(index=False))\n",
        "    test = testrow['target'].values[0] \n",
        "    print(\"the correct value was {}\".format(test))\n",
        "    firstPredict(testrow)\n",
        "    \n",
        "    old_stdout = sys.stdout\n",
        "    result = StringIO()\n",
        "    sys.stdout = result\n",
        "    firstPredict(testrow)\n",
        "    sys.stdout = old_stdout\n",
        "    \n",
        "    result_string = result.getvalue()\n",
        "    a, b, c, last = result_string.split()\n",
        "    #guess = int(last)\n",
        "    #print(\"last is {}\".format(last))\n",
        "    count = count + 1\n",
        "    if str(test) == str(last):\n",
        "      correct = correct + 1\n",
        "      print (\"I predicted correctly\")\n",
        "    else:\n",
        "      print(\"I was wrong\")\n",
        "    testdata = testdata.drop([0,0]).reset_index(drop=True)\n",
        "    print(\"\\n\")\n",
        "  print(\"Total rows was {} and I predicted {} correctly\".format(count, correct))\n",
        "  print(\"My accuracy is {}%\".format(correct/count * 100))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KY1ECmnUywNL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "####################################################\n",
        "#######this is where we finally call things ########\n",
        "####################################################"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JgFuK6Ps2K6m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#You will need to clear the training data if you want to 'train' different data\n",
        "try: \n",
        "  clearTrainedData()\n",
        "except:\n",
        "  print(\"nothing to clear\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TFsEuNs6y6H0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#this is to build the tree\n",
        "#the first parameter is the training dataframe\n",
        "#the second parameter is the depth to start at, which is always 1\n",
        "#the third parameter is the max depth you want to trim the decision tree to\n",
        "subsequent_split(df, 1, 5)    #put 1 more than the max depth you want"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GG-snUP_9fTt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# when you have the target variable in the testing data you can check\n",
        "# the accuracy of the data\n",
        "testDataAccuracy(testdf)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SrhRFdozozCS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# when you dont have the target variable in the testing data you can check\n",
        "# the prediction\n",
        "testData(testdf)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i8zSrCnqS3Es",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# you can print the dictionaries for each depth of the tree (as you desire)\n",
        "import pprint\n",
        "pprint.pprint(depth1)\n",
        "pprint.pprint(depth2)\n",
        "pprint.pprint(depth3)\n",
        "pprint.pprint(depth4)\n",
        "pprint.pprint(depth5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ISzhqah26-KQ",
        "colab_type": "text"
      },
      "source": [
        "##Evaluation\n",
        "\n",
        "\n",
        "**Report Execution on Data**\n",
        "\n",
        "Report Execution on Data\n",
        "When executing the code, we store the decision tree into a series of dictionaries that record the depth, feature, value, left (less than or equal to value) /right (greater than value) and prediction (None if not a leaf node). \n",
        "\n",
        "An example output from running the subsequent_split() method with a max depth of 4 on the iris dataset prints the following output:\n",
        "\n",
        "    I am working at depth 1\n",
        "       for petal length (cm) <= 1.9, we predict 0\n",
        "     I am now branching for petal length (cm) > 1.9\n",
        "       I am working at depth 2\n",
        "       i am now branching for petal width (cm) <= 1.7\n",
        "         I am working at depth 3\n",
        "         i am now branching for petal length (cm) <= 4.9\n",
        "           I am working at depth 4\n",
        "           for petal width (cm) <= 1.6, we predict 1\n",
        "           I am working at depth 4\n",
        "           for petal width (cm) > 1.6, we predict 2\n",
        "         I am now branching for petal length (cm) > 4.9\n",
        "           I am working at depth 4\n",
        "           for petal width (cm) <= 1.5, we predict 2\n",
        "           for petal width (cm) > 1.5, we predict 2\n",
        "       I am now branching for petal width (cm) > 1.7\n",
        "         I am working at depth 3\n",
        "         i am now branching for petal length (cm) <= 4.8\n",
        "           I am working at depth 4\n",
        "           for sepal length (cm) <= 5.9, we predict 1\n",
        "           I am working at depth 4\n",
        "           for sepal length (cm) > 5.9, we predict 2\n",
        "         I am working at depth 3\n",
        "         for petal length (cm) > 4.8, we predict 2\n",
        " \n",
        "For each split we see the depth of the decision tree, the rule being created and predicted target value (if exists).\n",
        "\n",
        "In order to highlight the splits being made, below is the first split value rule. \n",
        "\n",
        "    I am working at depth 1\n",
        "     for petal length (cm) <= 1.9, we predict 0\n",
        "   \n",
        "We see that if petal length (cm) is below 1.9 it is predicted to be target value 0. When plotting the values on a scatter graph below, we see that target value 0 (red) is clustered below petal length (cm) 1.9.\n",
        "\n",
        "(Please run the code block below)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XR8RZpWg9Imf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Run this for the scatter plot highlighting the split value\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "dataset = datasets.load_iris()\n",
        "\n",
        "df = pd.DataFrame(dataset['data'])\n",
        "df.columns = [name[:] for name in dataset['feature_names']] \n",
        "df['target'] = dataset['target']\n",
        "df.plot.scatter(x ='petal width (cm)', y='petal length (cm)', c='target', colormap='rainbow_r')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LvyB9CRMCqrk",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "**Report Testing**\n",
        "\n",
        "When evaluating the success of the CART decision tree model, we have the ability  to compare the actual and predicted target variable of each dataset instance. An example output is as follows:\n",
        " \n",
        "    For a Correctly predicted row\n",
        "    for the row:\n",
        "    sepal length(cm)  sepal width(cm)  petal length(cm)  petal width(cm)  target\n",
        "          6.1               2.8                4.7              1.2          1\n",
        "    the correct value was 1\n",
        "    the prediction is 1\n",
        "    I predicted correctly\n",
        "\n",
        "    For an Incorrectly predicted row\n",
        "    for the row:\n",
        "     sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)  target\n",
        "          6.3                 2.5                5.0               1.9            2\n",
        "    the correct value was 2\n",
        "    the prediction is 1\n",
        "    I was wrong\n",
        " \n",
        "After each row displayed, a final accuracy score is printed\n",
        "\n",
        "    Total rows was 30 and I predicted 24 correctly\n",
        "    My accuracy is 80.0%\n",
        "\n",
        "We can clearly see that if the target and prediction value match. A simple percentage calculation is used to calculate accuracy:\n",
        "\n",
        "(number of correct predictions) / (total instances) * 100\n",
        " \n",
        "Overall accuracy is an appropriate metric for the Iris dataset since the target variable is balanced across the three classes.  For other datasets, different performance metrics may be needed.\n",
        "\n",
        "\n",
        "**Efficiency Analysis**\n",
        "\n",
        "When comparing the computation run time between each dataset, we can observe how the size of the dataset is proportional to the number of observable features.\n",
        "The following test demonstrates how the number of rows and columns affect the CPU runtime in google Colab. With the ‘%timeit’ command we can track the time it takes to build a decision tree for each available dataset. It can be seen that the more rows and columns (with equal depth) increases the CPU runtime.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x6X5FRHdEuni",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# run this code to see the %timeit efficiency analysis table\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "df = pd.DataFrame(columns = [\"Dataset (sklearn.database) \", \"Rows (Instances)\", \"Columns (features)\", \"MaxDepth\", \"CPU runtime (%time)\"], \n",
        "                  data=[[\"iris\", 150, 4, 5, \"total: 2.08 s\"],\n",
        "                        [\"wine\", 178, 13, 5, \"total: 25.3 s\"],\n",
        "                        [\"diabetes\", 442, 10, 5, \"total: 31.9 s\"],\n",
        "                        [\"boston\", 506, 13, 5, \"total: 55.3 s\"],\n",
        "                        [\"breast_cancer\", 569, 30, 5, \"total: 4min 58s\"]])\n",
        "\n",
        "df"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wRVf_d9-Jr7g",
        "colab_type": "text"
      },
      "source": [
        "It is quite intuitive to understand that more data would equal more computation time. It is worth noting, however, that the number of features scales the time significantly more than the number of rows. This is due to the need to consider each feature across each row for the best possible split point and therefore requires more computation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G-FTRF_s85K_",
        "colab_type": "text"
      },
      "source": [
        "## Conclusion\n",
        "\n",
        "\n",
        "* Discuss Reflections\n",
        "\n",
        "The implementation of the CART decision tree algorithm has demonstrated how to generalize classification of instances based on features being correlated to a target class. In statistics, overfitting is the production of an analysis that corresponds too closely or exactly to a particular set of data, and may therefore fail to predict future observations reliably (Tetko et all 1995). The problem of overfitting is identified and proposed to be managed through or generalization (Friedman 1999). \n",
        "\n",
        "This project allows a user to experiment with the tree depth and how it affects the accuracy of a prediction.This highlights the effect of depth control as a requires management in order to avoid overfitting and maintain generalization.\n",
        "\n",
        "The computation efficiency has also been demonstrated showing that cpu run time  is proportional to the number of instances, features and depth. This project implementation may not be the most efficient method but it highlights the impact of datasets with different number of rows and features. This can be further explored with the maxdepth parameter and observe its effect. In our learnings we discovered using a class instead of a dictionary would improve computational efficiency.\n",
        "\n",
        "\n",
        "* Propose Possible Improvements\n",
        "\n",
        "A number of possible improvements have been identified that if implemented would improve the performance and efficiency of the code. Firstly, if we defined a ‘node’ class instead of using dictionaries the performance would increase exponentially due to the fact that we store all previous dictionary entries into each subsequent dictionary depth. \n",
        "\n",
        "Another improvement would be implementing other splitting criteria (e.g. entropy & information gain) so that comparisons can be made between their performance. If additional user parameters were included that improved experimentation, such as pruning or setting a gini impurity threshold greater generalization could be achieved. \n",
        "\n",
        "Further methods could also be implemented to work out the optimal depth of the tree than manually determining it, such as doing a grid search. \n",
        "\n",
        "If the algorithm is implemented on different data sets, which are not as balanced as the Iris dataset, further improvements could be made by weighing the minority class more heavily.\n",
        "Also, this CART decision tree implementation could be expanded into a random forest algorithm (or gradient boosted tree) by iterating a number of tree builds and picking the best scores for a better chance at building a more accurate model.\n",
        "\n",
        "Finally, the test data was taken as a subset of the training data. Ideally, these should be independent of each other. For demonstration purposes we did not split, and as such the results are over-fitted, but nonetheless currently shows a demonstration of the algorithm working. \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gLV7jwde8-oX",
        "colab_type": "text"
      },
      "source": [
        "## Ethical (200)\n",
        "\n",
        "* Discuss the social/ethical aspect of the project\n",
        "\n",
        "Ethics refer to the principles of right and wrong that individuals use to make choices that\n",
        "guide their behaviors (Wiley 2017). The CART decision tree implementation project adopts the utilitarianism model, which aims to do what causes the most good for the most people (Anderson & Anderson 2011). This approach is designed into the algorithm by making decisions based on the best possible split point and aims to guide the user on the potential classification of an instance based on historically observed features.\n",
        "\n",
        "* Consider how the technique could be misused\n",
        "\n",
        "The algorithm could be misused if a user considers the output as an absolute truth. If the predicted target value was to be treated as evidence for future decisions it has the potential to be misguided due to the probabilistic nature of how a prediction is made (Willis 2014). This can be especially pronounced when used by users less well versed with the algorithm, such as with higher management or stakeholders. Such users will easily comprehend how the decision tree works, but may apply hard and fast rules to something which should have such rules.\n",
        "\n",
        "The algorithm can also lead to socially unacceptable generalisations and biasedness, such as making decisions based on race, sex or religion. This can cause certain features to be prejudicial when making decisions. This can cause further dilemmas such as whether a data scientist should use such discriminatory features when classifying data (e.g. is it ethical that race be a feature in determining fraud?). \n",
        "\n",
        "Ultimately, decisions may be being made without considering unobserved features that would contribute greatly to future classifications. For example, it might not be race being the determinative of crime rates, but socio-economic backgrounds. With the algorithm, the generalisations of features may not be adequately showing this."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TdspPnHH9Cn3",
        "colab_type": "text"
      },
      "source": [
        "## Video Pitch\n",
        "\n",
        "Video url: https://youtu.be/mEefphtMhcc\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NtZqh3f9zW9b",
        "colab_type": "text"
      },
      "source": [
        "## References\n",
        "\n",
        "Anderson, M. and Anderson, S.L. eds., 2011. Machine ethics. Cambridge University Press.\n",
        "\n",
        "Breiman, L., Friedman, J., Olshen, R. and Stone, C., 1984. Classification and regression trees. Wadsworth Int. Group, 37(15), pp.237-251.\n",
        "\n",
        "Brownlee, J., 2016. Master Machine Learning Algorithms: discover how they work and implement them from scratch. Machine Learning Mastery.\n",
        "\n",
        "Carneiro, T., Da Nóbrega, R.V.M., Nepomuceno, T., Bian, G.B., De Albuquerque, V.H.C. and Reboucas \n",
        "\n",
        "Filho, P.P., 2018. Performance Analysis of Google Colaboratory as a Tool for Accelerating Deep Learning Applications. IEEE Access, 6, pp.61677-61685.\n",
        "\n",
        "Friedman, J.H., 1999. Greedy function approximation: a gradient boosting machine. Annals of statistics, pp.1189-1232.\n",
        "\n",
        "Hastie, T., Tibshirani, R., Friedman, J. and Franklin, J., 2005. The elements of statistical learning: data mining, inference and prediction. The Mathematical Intelligencer, 27(2), pp.83-85.\n",
        "\n",
        "Louppe, G., 2016. An introduction to Machine Learning with Scikit-Learn.\n",
        "\n",
        "McKinney, W., 2011. pandas: a foundational Python library for data analysis and statistics. Python for High Performance and Scientific Computing, 14.\n",
        "\n",
        "Peng, W., Chen, J. and Zhou, H., 2009. An implementation of ID3-decision tree learning algorithm. From web.arch.usyd.edu.au/wpeng/DecisionTree2.pdf viewed 21/09/2019\n",
        "\n",
        "Singh, S. & Gupta, P., 2014. Comparative study ID3, cart and C4. 5 decision tree algorithm: a survey. International Journal of Advanced Information Science and Technology (IJAIST), 27(27), pp.97-103.\n",
        "\n",
        "Strong, D.M., Lee, Y.W. and Wang, R.Y., 1997. Data quality in context. Communications of the ACM, 40(5), pp.103-110.\n",
        "\n",
        "Tetko, I.V., Livingstone, D.J. and Luik, A.I., 1995. Neural network studies. 1. Comparison of overfitting and overtraining. Journal of chemical information and computer sciences, 35(5), pp.826-833.\n",
        "\n",
        "Timofeev, R., 2004. Classification and regression trees (CART) theory and applications. Humboldt University, Berlin.\n",
        "\n",
        "Wiley J. & Sons, Inc. 2017, ‘Management Information Systems’, Chapter 6, p2.\n",
        "\n",
        "Willis, J.E., 2014. Learning analytics and ethics: A framework beyond utilitarianism. Educause Review.\n",
        "\n"
      ]
    }
  ]
}