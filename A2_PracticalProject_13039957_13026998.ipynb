{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of Untitled2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DamienSmith/UTS_ML2019_ID13039957/blob/master/A2_PracticalProject_13039957_13026998.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TWf4FTWL7mhY",
        "colab_type": "text"
      },
      "source": [
        "# A2: Algorithm Implementation | 31005 | Advanced Data Analytics\n",
        "Student: 13039957 & 13026998\n",
        "\n",
        "Link to Github:\n",
        "https://github.com/DamienSmith/UTS_ML2019_ID13039957/blob/master/A2_PracticalProject_13039957_13026998.ipynb"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N0-ciK386-52",
        "colab_type": "text"
      },
      "source": [
        "##Introduction - a project overview (100)\n",
        "\n",
        "\n",
        "**The Decision Tree (CART) Algorithm** \n",
        "\n",
        "A decision tree is a tree in which each branch node represents a choice between a number of alternatives, and each leaf node represents a classification or decision (Singh & Gupta 2014). Starting with a root node rules are applied that split and generate new nodes recursively according to a learning algorithm resulting in a decision tree where each edge represents a question and its outcome (Peng, Chen & Zhou 2009). Common decision tree algorithms include ID3, CART and C4.5 which each use different splitting criteria for splitting a node at each level to find a node with the least possible solutions (Singh & Gupta 2014). \n",
        "\n",
        "For this project, the CART (Classification and Regression Trees) algorithm will be implemented from scratch. The representation of the CART model is a binary tree where each node can have zero, one or two child nodes (Brownlee 2016). All input variables and all possible split points are evaluated and chosen in a greedy manner based on the cost function. The Gini cost function is used which provides an purity score which represents how mixed the training data assigned to each node is. A split is made on the best score at each level of depth. Splitting continues until nodes contain a minimum number of training examples or a maximum tree depth is reached. Once created, a tree can be navigated with a new row of data following each branch with the splits until a final prediction is made.\n",
        "\n",
        "**Define Input/Output** \n",
        "* What data you use and what youâ€™re getting out\n",
        "* the format of the I/O data\n",
        "\n",
        "The data we are using is the iris dataset, however, this algorithm is designed to handle any dataset as long as it is inputted as a pandas dataframe. Pandas is a Python library providing integrated, intuitive routines for performing common data manipulations and analysis on data sets (McKinney 2011). After running the algorithm, the predicted value of each row of the testing dataframe will be printed in the command line. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-076P8Yx6-iu",
        "colab_type": "text"
      },
      "source": [
        "##Exploration (300)\n",
        "\n",
        "**Identify Challenges**\n",
        "\n",
        "* Highlight the practical significance of the project\n",
        "* What problems exist and how to manage them\n",
        "  * e.g. memory management, time efficiency, 'advanced functions' (eg parallelism) \n",
        "\n",
        "**Design Data Structures**\n",
        "\n",
        "* Design/planning of research/development is clear and logical\n",
        "* Cover these:\n",
        "  * data acquisition\n",
        "  * quality control\n",
        "  * modelling technicques\n",
        "  * evaluation method & criteria\n",
        "\n",
        "**Plan Data Models and Tests**\n",
        "\n",
        "* Logical design (correct, efficient and practically complete)\n",
        "* Evaluation method/Testing (Compare and consider alternatives)\n",
        "* Must be able to run code through collab \n",
        "  * i.e. data load and python libraries need to work in Collab\n",
        "\n",
        "\n",
        "**Possible alternatives**\n",
        "\n",
        "* Algorithm Tuning. The application of CART to the Bank Note dataset was not tuned. Experiment with different parameter values and see if you can achieve better performance.\n",
        "\n",
        "* Cross Entropy. Another cost function for evaluating splits is cross entropy (logloss). You could implement and experiment with this alternative cost function.\n",
        "\n",
        "* Tree Pruning. An important technique for reducing overfitting of the training dataset is to prune the trees. Investigate and implement tree pruning methods.\n",
        "\n",
        "* Categorical Dataset. The example was designed for input data with numerical or ordinal input attributes, experiment with categorical input data and splits that may use equality instead of ranking.\n",
        "\n",
        "* Regression. Adapt the tree for regression using a different cost function and method for creating terminal nodes.\n",
        "\n",
        "* More Datasets. Apply the algorithm to more datasets on the UCI Machine Learning Repository.\n",
        "\n",
        "\n",
        "\n",
        "There several choices there is historically ID3 the Iterative Dichotomiser 3, C5 which I believe stands for Classifier as development of ID3. CART, which is for classification and regression tree. It's very common in Universal Choice. CHAID, which is for CHi-squared Automatic Interaction Detectior. MARS, which is multivariate adaptive regression splines. And the one that I'm going to be using Conditional Inference trees. Now decision trees in general have some pros and cons.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dv3kz8Cx6-bm",
        "colab_type": "text"
      },
      "source": [
        "##Methodology (100 ex comments)\n",
        "\n",
        "[code here]\n",
        "\n",
        "**Build and Train Data Models**\n",
        "\n",
        "* Comments connect the code to the algorithm steps\n",
        "* \n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JuFQehMoR0uz",
        "colab_type": "text"
      },
      "source": [
        "## Decision Tree Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P9F55nV5LWxV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## \n",
        "##    A2: Algorithm Implementation | 31005 | Advanced Data Analytics\n",
        "##    \n",
        "##    Authors: Rae Ho (13026998) & Damien Smith (13039957)\n",
        "##    Goals: - Implement Decision Tree Algorithm\n",
        "##           - Build & Train a model\n",
        "##           - Explore/Compare different parameters \n",
        "##    Code: Python 3\n",
        "##    Github: https://github.com/DamienSmith/UTS_ML2019_ID13039957/blob/master/A2_PracticalProject_13039957_13026998.ipynb\n",
        "##\n",
        "##\n",
        "\n",
        "# Import Libraries\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import pandas_profiling as pp\n",
        "from sklearn import datasets\n",
        "\n",
        "# Load Data\n",
        "# Just some test data\n",
        "dataset = datasets.load_iris()\n",
        "X = dataset.data[:, [0, 2]]\n",
        "y = dataset.target\n",
        "dataset\n",
        "# Get dataset from github\n",
        "# url = 'https://raw.githubusercontent.com/DamienSmith/UTS_ML2019_ID13039957/master/characters.csv'\n",
        "# df = pd.read_csv(url)\n",
        "\n",
        "#Data profiling\n",
        "#pp.ProfileReport(dataset)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xv4S9QNXQsBJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Define Gini_index() Function \n",
        "# Calculate the Gini index for a split dataset\n",
        "\n",
        "def gini_index(groups, classes):\n",
        "\t# count all samples at split point\n",
        "\tn_instances = float(sum([len(group) for group in groups]))\n",
        "\t# sum weighted Gini index for each group\n",
        "\tgini = 0.0\n",
        "\tfor group in groups:\n",
        "\t\tsize = float(len(group))\n",
        "\t\t# avoid divide by zero\n",
        "\t\tif size == 0:\n",
        "\t\t\tcontinue\n",
        "\t\tscore = 0.0\n",
        "\t\t# score the group based on the score for each class\n",
        "\t\tfor class_val in classes:\n",
        "\t\t\tp = [row[-1] for row in group].count(class_val) / size\n",
        "\t\t\tscore += p * p\n",
        "\t\t# weight the group score by its relative size\n",
        "\t\tgini += (1.0 - score) * (size / n_instances)\n",
        "\treturn gini\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-JyFVr7NRfop",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Define test_split() Function.\n",
        "# Separate the dataset into two lists of rows given the index of an attribute and a split value for that attribute.\n",
        "# Once we have the two groups, we can then use our Gini score above to evaluate the cost of the split.\n",
        "# Note: The right group contains all rows with a value at the index above or equal to the split value.\n",
        "\n",
        "# Split a dataset based on an attribute and an attribute value\n",
        "def test_split(index, value, dataset):\n",
        "\tleft, right = list(), list()\n",
        "\tfor row in dataset:\n",
        "\t\tif row[index] < value:\n",
        "\t\t\tleft.append(row)\n",
        "\t\telse:\n",
        "\t\t\tright.append(row)\n",
        "\treturn left, right\n",
        "\n",
        "\n",
        "# Splitting a dataset involves iterating over each row, checking if the attribute value is below or above the split value and assigning it to the left or right group respectively."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iy5EjwWHSKzZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Define get_split() Function.\n",
        "# With the Gini function above and the test split function we now have everything we need to evaluate splits.\n",
        "# Given a dataset, we must check every value on each attribute as a candidate split, evaluate the cost of the split and find the best possible split we could make.\n",
        "# Once the best split is found, we can use it as a node in our decision tree\n",
        "\n",
        "def get_split(dataset):\n",
        "\tclass_values = list(set(row[-1] for row in dataset))\n",
        "\tb_index, b_value, b_score, b_groups = 999, 999, 999, None\n",
        "\tfor index in range(len(dataset[0])-1):\n",
        "\t\tfor row in dataset:\n",
        "\t\t\tgroups = test_split(index, row[index], dataset)\n",
        "\t\t\tgini = gini_index(groups, class_values)\n",
        "\t\t\tprint('X%d < %.3f Gini=%.3f' % ((index+1), row[index], gini))\n",
        "\t\t\tif gini < b_score:\n",
        "\t\t\t\tb_index, b_value, b_score, b_groups = index, row[index], gini, groups\n",
        "\treturn {'index':b_index, 'value':b_value, 'groups':b_groups}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oO8C9l0hXOSL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Calculate the Gini index for a split dataset\n",
        "def gini_index(groups, classes):\n",
        "\t# count all samples at split point\n",
        "\tn_instances = float(sum([len(group) for group in groups]))\n",
        "\t# sum weighted Gini index for each group\n",
        "\tgini = 0.0\n",
        "\tfor group in groups:\n",
        "\t\tsize = float(len(group))\n",
        "\t\t# avoid divide by zero\n",
        "\t\tif size == 0:\n",
        "\t\t\tcontinue\n",
        "\t\tscore = 0.0\n",
        "\t\t# score the group based on the score for each class\n",
        "\t\tfor class_val in classes:\n",
        "\t\t\tp = [row[-1] for row in group].count(class_val) / size\n",
        "\t\t\tscore += p * p\n",
        "\t\t# weight the group score by its relative size\n",
        "\t\tgini += (1.0 - score) * (size / n_instances)\n",
        "\treturn gini\n",
        "\n",
        "# Split a dataset based on an attribute and an attribute value\n",
        "def test_split(index, value, dataset):\n",
        "\tleft, right = list(), list()\n",
        "\tfor row in dataset:\n",
        "\t\tif row[index] < value:\n",
        "\t\t\tleft.append(row)\n",
        "\t\telse:\n",
        "\t\t\tright.append(row)\n",
        "\treturn left, right\n",
        "\n",
        "# Select the best split point for a dataset\n",
        "def get_split(dataset):\n",
        "\tclass_values = list(set(row[-1] for row in dataset))\n",
        "\tb_index, b_value, b_score, b_groups = 999, 999, 999, None\n",
        "\tfor index in range(len(dataset[0])-1):\n",
        "\t\tfor row in dataset:\n",
        "\t\t\tgroups = test_split(index, row[index], dataset)\n",
        "\t\t\tgini = gini_index(groups, class_values)\n",
        "\t\t\tprint('X%d < %.3f Gini=%.3f' % ((index+1), row[index], gini))\n",
        "\t\t\tif gini < b_score:\n",
        "\t\t\t\tb_index, b_value, b_score, b_groups = index, row[index], gini, groups\n",
        "\treturn {'index':b_index, 'value':b_value, 'groups':b_groups}\n",
        " \n",
        "split = get_split(dataset)\n",
        "print('Split: [X%d < %.3f]' % ((split['index']+1), split['value']))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8YGS3CuVZD4G",
        "colab_type": "code",
        "outputId": "3d0fa00c-e3e2-4d9d-a520-473c11971fd3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281
        }
      },
      "source": [
        "# plot scatter plot - needs work\n",
        "x = [row[0] for row in dataset]\n",
        "y = [row[1] for row in dataset]\n",
        "\n",
        "data = x,y\n",
        "colors = (\"red\", \"blue\")\n",
        "groups = (\"X1\", \"X2\")\n",
        "\n",
        "# Create plot\n",
        "fig = plt.figure()\n",
        "ax = fig.add_subplot(1, 1, 1)\n",
        "\n",
        "for data, color, group in zip(data, colors, groups):\n",
        "  x = data\n",
        "  y = data\n",
        "  ax.scatter(x, y, alpha=0.8, c=color, edgecolors='none', s=30, label=group)\n",
        "\n",
        "plt.title('Scatter plot')\n",
        "plt.legend(loc=2)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAEICAYAAABPgw/pAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n/wGWoQkhyhDQzQAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W8Vaj5HThPEB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# We need to control the size of a tree by defining the depth and the number of rows that a node will run.\n",
        "\n",
        "# We use user-defined arguments to define tree building procedure.\n",
        "# Maximum Tree Depth. This is the maximum number of nodes from the root node of the tree. Once a maximum depth of the tree is met, we must stop splitting adding new nodes. Deeper trees are more complex and are more likely to overfit the training data.\n",
        "# Minimum Node Records. This is the minimum number of training patterns that a given node is responsible for. Once at or below this minimum, we must stop splitting and adding new nodes. Nodes that account for too few training patterns are expected to be too specific and are likely to overfit the training data.\n",
        "\n",
        "# There is one more condition. It is possible to choose a split in which all rows belong to one group. In this case, we will be unable to continue splitting and adding child nodes as we will have no records to split on one side or another.\n",
        "\n",
        "# Now we have some ideas of when to stop growing the tree. When we do stop growing at a given point, that node is called a terminal node and is used to make a final prediction.\n",
        "\n",
        "# This is done by taking the group of rows assigned to that node and selecting the most common class value in the group. This will be used to make predictions.\n",
        "\n",
        "\n",
        "# Define to_terminal() Function\n",
        "# Select a class value for a group of rows. It returns the most common output value in a list of rows.\n",
        "def to_terminal(group):\n",
        "\toutcomes = [row[-1] for row in group]\n",
        "\treturn max(set(outcomes), key=outcomes.count)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-FkenWeghnVf",
        "colab_type": "code",
        "outputId": "69d9c300-1cf6-4bc4-bf5b-5edf51289608",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "# # Split a dataset based on an attribute and an attribute value\n",
        "def test_split(index, value, dataset):\n",
        "\tleft, right = list(), list()\n",
        "\tfor row in dataset:\n",
        "\t\tif row[index] < value:\n",
        "\t\t\tleft.append(row)\n",
        "\t\telse:\n",
        "\t\t\tright.append(row)\n",
        "\treturn left, right\n",
        " \n",
        "# Calculate the Gini index for a split dataset\n",
        "def gini_index(groups, classes):\n",
        "\t# count all samples at split point\n",
        "\tn_instances = float(sum([len(group) for group in groups]))\n",
        "\t# sum weighted Gini index for each group\n",
        "\tgini = 0.0\n",
        "\tfor group in groups:\n",
        "\t\tsize = float(len(group))\n",
        "\t\t# avoid divide by zero\n",
        "\t\tif size == 0:\n",
        "\t\t\tcontinue\n",
        "\t\tscore = 0.0\n",
        "\t\t# score the group based on the score for each class\n",
        "\t\tfor class_val in classes:\n",
        "\t\t\tp = [row[-1] for row in group].count(class_val) / size\n",
        "\t\t\tscore += p * p\n",
        "\t\t# weight the group score by its relative size\n",
        "\t\tgini += (1.0 - score) * (size / n_instances)\n",
        "\treturn gini\n",
        " \n",
        "# Select the best split point for a dataset\n",
        "def get_split(dataset):\n",
        "\tclass_values = list(set(row[-1] for row in dataset))\n",
        "\tb_index, b_value, b_score, b_groups = 999, 999, 999, None\n",
        "\tfor index in range(len(dataset[0])-1):\n",
        "\t\tfor row in dataset:\n",
        "\t\t\tgroups = test_split(index, row[index], dataset)\n",
        "\t\t\tgini = gini_index(groups, class_values)\n",
        "\t\t\tif gini < b_score:\n",
        "\t\t\t\tb_index, b_value, b_score, b_groups = index, row[index], gini, groups\n",
        "\treturn {'index':b_index, 'value':b_value, 'groups':b_groups}\n",
        " \n",
        "# Create a terminal node value\n",
        "def to_terminal(group):\n",
        "\toutcomes = [row[-1] for row in group]\n",
        "\treturn max(set(outcomes), key=outcomes.count)\n",
        " \n",
        "# Create child splits for a node or make terminal\n",
        "def split(node, max_depth, min_size, depth):\n",
        "\tleft, right = node['groups']\n",
        "\tdel(node['groups'])\n",
        "\t# check for a no split\n",
        "\tif not left or not right:\n",
        "\t\tnode['left'] = node['right'] = to_terminal(left + right)\n",
        "\t\treturn\n",
        "\t# check for max depth\n",
        "\tif depth >= max_depth:\n",
        "\t\tnode['left'], node['right'] = to_terminal(left), to_terminal(right)\n",
        "\t\treturn\n",
        "\t# process left child\n",
        "\tif len(left) <= min_size:\n",
        "\t\tnode['left'] = to_terminal(left)\n",
        "\telse:\n",
        "\t\tnode['left'] = get_split(left)\n",
        "\t\tsplit(node['left'], max_depth, min_size, depth+1)\n",
        "\t# process right child\n",
        "\tif len(right) <= min_size:\n",
        "\t\tnode['right'] = to_terminal(right)\n",
        "\telse:\n",
        "\t\tnode['right'] = get_split(right)\n",
        "\t\tsplit(node['right'], max_depth, min_size, depth+1)\n",
        "    \n",
        "# Build a decision tree\n",
        "def build_tree(train, max_depth, min_size):\n",
        "\troot = get_split(train)\n",
        "\tsplit(root, max_depth, min_size, 1)\n",
        "\treturn root\n",
        " \n",
        "# Print a decision tree\n",
        "def print_tree(node, depth=0):\n",
        "\tif isinstance(node, dict):\n",
        "\t\tprint('%s[X%d < %.3f]' % ((depth*' ', (node['index']+1), node['value'])))\n",
        "\t\tprint_tree(node['left'], depth+1)\n",
        "\t\tprint_tree(node['right'], depth+1)\n",
        "\telse:\n",
        "\t\tprint('%s[%s]' % ((depth*' ', node)))\n",
        "\n",
        "tree = build_tree(dataset, 1, 1)\n",
        "print_tree(tree)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[X1 < 6.642]\n",
            " [0]\n",
            " [1]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e4yODkiVhnO9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## show some experimenting with different 'max depth' parameters\n",
        "\n",
        "##\n",
        "\n",
        "##\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8dg4c-2LiL2P",
        "colab_type": "code",
        "outputId": "56f6ebf8-2a7a-4108-e4f9-bbb4d73afa21",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "source": [
        "# Make a prediction with a decision tree\n",
        "def predict(node, row):\n",
        "\tif row[node['index']] < node['value']:\n",
        "\t\tif isinstance(node['left'], dict):\n",
        "\t\t\treturn predict(node['left'], row)\n",
        "\t\telse:\n",
        "\t\t\treturn node['left']\n",
        "\telse:\n",
        "\t\tif isinstance(node['right'], dict):\n",
        "\t\t\treturn predict(node['right'], row)\n",
        "\t\telse:\n",
        "\t\t\treturn node['right']\n",
        "    \n",
        "#  predict with a stump\n",
        "stump = {'index': 0, 'right': 1, 'value': 6.642287351, 'left': 0}\n",
        "for row in dataset:\n",
        "\tprediction = predict(stump, row)\n",
        "\tprint('Predicted=%d, Result=%d' % (row[-1], prediction))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Predicted=0, Result=0\n",
            "Predicted=0, Result=0\n",
            "Predicted=0, Result=0\n",
            "Predicted=0, Result=0\n",
            "Predicted=0, Result=0\n",
            "Predicted=1, Result=1\n",
            "Predicted=1, Result=1\n",
            "Predicted=1, Result=1\n",
            "Predicted=1, Result=1\n",
            "Predicted=1, Result=1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fk89B4XCiy6Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Comparisons\n",
        "\n",
        "## Regression?\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "92elhOEKU2uw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#\n",
        "# ID3 Decision Tree (Information Gain)\n",
        "#\n",
        "\n",
        "def find_entropy(df):\n",
        "    Class = df.keys()[-1]   #To make the code generic, changing target variable class name\n",
        "    entropy = 0\n",
        "    values = df[Class].unique()\n",
        "    for value in values:\n",
        "        fraction = df[Class].value_counts()[value]/len(df[Class])\n",
        "        entropy += -fraction*np.log2(fraction)\n",
        "    return entropy\n",
        "  \n",
        "  \n",
        "def find_entropy_attribute(df,attribute):\n",
        "  Class = df.keys()[-1]   #To make the code generic, changing target variable class name\n",
        "  target_variables = df[Class].unique()  #This gives all 'Yes' and 'No'\n",
        "  variables = df[attribute].unique()    #This gives different features in that attribute (like 'Hot','Cold' in Temperature)\n",
        "  entropy2 = 0\n",
        "  for variable in variables:\n",
        "      entropy = 0\n",
        "      for target_variable in target_variables:\n",
        "          num = len(df[attribute][df[attribute]==variable][df[Class] ==target_variable])\n",
        "          den = len(df[attribute][df[attribute]==variable])\n",
        "          fraction = num/(den+eps)\n",
        "          entropy += -fraction*log(fraction+eps)\n",
        "      fraction2 = den/len(df)\n",
        "      entropy2 += -fraction2*entropy\n",
        "  return abs(entropy2)\n",
        "\n",
        "\n",
        "def find_winner(df):\n",
        "    Entropy_att = []\n",
        "    IG = []\n",
        "    for key in df.keys()[:-1]:\n",
        "#         Entropy_att.append(find_entropy_attribute(df,key))\n",
        "        IG.append(find_entropy(df)-find_entropy_attribute(df,key))\n",
        "    return df.keys()[:-1][np.argmax(IG)]\n",
        "  \n",
        "  \n",
        "def get_subtable(df, node,value):\n",
        "  return df[df[node] == value].reset_index(drop=True)\n",
        "\n",
        "\n",
        "def buildTree(df,tree=None): \n",
        "    Class = df.keys()[-1]   #To make the code generic, changing target variable class name\n",
        "    \n",
        "    #Here we build our decision tree\n",
        "\n",
        "    #Get attribute with maximum information gain\n",
        "    node = find_winner(df)\n",
        "    \n",
        "    #Get distinct value of that attribute e.g Salary is node and Low,Med and High are values\n",
        "    attValue = np.unique(df[node])\n",
        "    \n",
        "    #Create an empty dictionary to create tree    \n",
        "    if tree is None:                    \n",
        "        tree={}\n",
        "        tree[node] = {}\n",
        "    \n",
        "   #We make loop to construct a tree by calling this function recursively. \n",
        "    #In this we check if the subset is pure and stops if it is pure. \n",
        "\n",
        "    for value in attValue:\n",
        "        \n",
        "        subtable = get_subtable(df,node,value)\n",
        "        clValue,counts = np.unique(subtable['Eat'],return_counts=True)                        \n",
        "        \n",
        "        if len(counts)==1:#Checking purity of subset\n",
        "            tree[node][value] = clValue[0]                                                    \n",
        "        else:        \n",
        "            tree[node][value] = buildTree(subtable) #Calling the function recursively \n",
        "                   \n",
        "    return tree"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T3wj_QJ8Yof3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def predict(inst,tree):\n",
        "    #This function is used to predict for any input variable \n",
        "    \n",
        "    #Recursively we go through the tree that we built earlier\n",
        "\n",
        "    for nodes in tree.keys():        \n",
        "        \n",
        "        value = inst[nodes]\n",
        "        tree = tree[nodes][value]\n",
        "        prediction = 0\n",
        "            \n",
        "        if type(tree) is dict:\n",
        "            prediction = predict(inst, tree)\n",
        "        else:\n",
        "            prediction = tree\n",
        "            break;                            \n",
        "        \n",
        "    return prediction"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "81X3QiHy949j",
        "colab": {}
      },
      "source": [
        "## Show comparison with a Large/small dataset\n",
        "\n",
        "## Show comparison of predictive accuracy between Decision Trees"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ISzhqah26-KQ",
        "colab_type": "text"
      },
      "source": [
        "##Evaluation (200)\n",
        "\n",
        "\n",
        "**Report Execution on Data**\n",
        "\n",
        "**Report Testing**\n",
        "\n",
        "**Efficiency Analysis**\n",
        "\n",
        "**Comparative Study**\n",
        "\n",
        "ideas:\n",
        "* how does each feature distribution look like? Are there any differences between feature distribution in train and test data?\n",
        "* are there any meaningful interactions between the features?\n",
        "* are there outliers and can they be explained?\n",
        "* are there missing values or duplicates? What are reasons for them?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G-FTRF_s85K_",
        "colab_type": "text"
      },
      "source": [
        "## Conclusion (100)\n",
        "\n",
        "\n",
        "Discuss Reflections\n",
        "\n",
        "Propose Possible Improvements\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gLV7jwde8-oX",
        "colab_type": "text"
      },
      "source": [
        "## Ethical (200)\n",
        "\n",
        "* Discuss the social/ethical aspect of the project\n",
        "  * adopt an ethical model (e.g. Ultitarian or Kantian)\n",
        "* Consider how the technique could be misused"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TdspPnHH9Cn3",
        "colab_type": "text"
      },
      "source": [
        "## Video Pitch\n",
        "\n",
        "[url to video]\n",
        "\n",
        "**Highlight Challenges and Effort**\n",
        "\n",
        "* Describe challenges and how the team addressed them.\n",
        "  * Python Skills\n",
        "    * Pandas, numpy, others?\n",
        "    * Dictionary (Hashmap)\n",
        "  * Code structure\n",
        "    * Generating 'depth' and 'terminal' flags in dictionary\n",
        "    * Handling looping (to avoid hardcoding for iris and adding flexibility to use any dataframe)\n",
        "  * Managing Assignment scope \n",
        "    * We initially wanted to run each cost function (ID3, CART and C4.5) (entropy? info gain?)\n",
        "    * We also wanted to turn it into a Random forest\n",
        "    * scoping this down into the time we had available was a challenge (while learning)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NtZqh3f9zW9b",
        "colab_type": "text"
      },
      "source": [
        "## References\n",
        "\n",
        "Brownlee, J., 2016. Master Machine Learning Algorithms: discover how they work and implement them from scratch. Machine Learning Mastery.\n",
        "\n",
        "McKinney, W., 2011. pandas: a foundational Python library for data analysis and statistics. Python for High Performance and Scientific Computing, 14.\n",
        "\n",
        "Peng, W., Chen, J. and Zhou, H., 2009. An implementation of ID3-decision tree learning algorithm. From web.arch.usyd.edu.au/wpeng/DecisionTree2.pdf viewed 21/09/2019\n",
        "\n",
        "Singh, S. & Gupta, P., 2014. Comparative study ID3, cart and C4. 5 decision tree algorithm: a survey. International Journal of Advanced Information Science and Technology (IJAIST), 27(27), pp.97-103.\n"
      ]
    }
  ]
}