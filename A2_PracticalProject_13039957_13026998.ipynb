{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of Untitled2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DamienSmith/UTS_ML2019_ID13039957/blob/master/A2_PracticalProject_13039957_13026998.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TWf4FTWL7mhY",
        "colab_type": "text"
      },
      "source": [
        "# A2: Algorithm Implementation | 31005 | Advanced Data Analytics\n",
        "Student: 13039957 & 13026998\n",
        "\n",
        "Link to Github:\n",
        "https://github.com/DamienSmith/UTS_ML2019_ID13039957/blob/master/A2_PracticalProject_13039957_13026998.ipynb"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N0-ciK386-52",
        "colab_type": "text"
      },
      "source": [
        "##Introduction - a project overview (100)\n",
        "\n",
        "\n",
        "**The Decision Tree (CART) Algorithm** \n",
        "\n",
        "A decision tree is a tree in which each branch node represents a choice between a number of alternatives, and each leaf node represents a classification or decision (Singh & Gupta 2014). Starting with a root node rules are applied that split and generate new nodes recursively according to a learning algorithm resulting in a decision tree where each edge represents a question and its outcome (Peng, Chen & Zhou 2009). Common decision tree algorithms include ID3, CART and C4.5 which each use different splitting criteria for splitting a node at each level to find a node with the least possible solutions (Singh & Gupta 2014). \n",
        "\n",
        "For this project, the CART (Classification and Regression Trees) algorithm will be implemented from scratch. The representation of the CART model is a binary tree where each node can have zero, one or two child nodes (Brownlee 2016). All input variables and all possible split points are evaluated and chosen in a greedy manner based on the cost function. The Gini cost function is used which provides an purity score which represents how mixed the training data assigned to each node is. A split is made on the best score at each level of depth. Splitting continues until nodes contain a minimum number of training examples or a maximum tree depth is reached. Once created, a tree can be navigated with a new row of data following each branch with the splits until a final prediction is made.\n",
        "\n",
        "**Define Input/Output** \n",
        "* What data you use and what youâ€™re getting out\n",
        "* the format of the I/O data\n",
        "\n",
        "The data we are using is the iris dataset, however, this algorithm is designed to handle any dataset as long as it is inputted as a pandas dataframe. Pandas is a Python library providing integrated, intuitive routines for performing common data manipulations and analysis on data sets (McKinney 2011). After running the algorithm, the predicted value of each row of the testing dataframe will be printed in the command line. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-076P8Yx6-iu",
        "colab_type": "text"
      },
      "source": [
        "##Exploration (300)\n",
        "\n",
        "**Identify Challenges**\n",
        "\n",
        "* Highlight the practical significance of the project\n",
        "* What problems exist and how to manage them\n",
        "  * e.g. memory management, time efficiency, 'advanced functions' (eg parallelism) \n",
        "\n",
        "**Design Data Structures**\n",
        "\n",
        "* Design/planning of research/development is clear and logical\n",
        "* Cover these:\n",
        "  * data acquisition\n",
        "  * quality control\n",
        "  * modelling technicques\n",
        "  * evaluation method & criteria\n",
        "\n",
        "**Plan Data Models and Tests**\n",
        "\n",
        "* Logical design (correct, efficient and practically complete)\n",
        "* Evaluation method/Testing (Compare and consider alternatives)\n",
        "* Must be able to run code through collab \n",
        "  * i.e. data load and python libraries need to work in Collab\n",
        "\n",
        "\n",
        "**Possible alternatives**\n",
        "\n",
        "* Algorithm Tuning. The application of CART to the Bank Note dataset was not tuned. Experiment with different parameter values and see if you can achieve better performance.\n",
        "\n",
        "* Cross Entropy. Another cost function for evaluating splits is cross entropy (logloss). You could implement and experiment with this alternative cost function.\n",
        "\n",
        "* Tree Pruning. An important technique for reducing overfitting of the training dataset is to prune the trees. Investigate and implement tree pruning methods.\n",
        "\n",
        "* Categorical Dataset. The example was designed for input data with numerical or ordinal input attributes, experiment with categorical input data and splits that may use equality instead of ranking.\n",
        "\n",
        "* Regression. Adapt the tree for regression using a different cost function and method for creating terminal nodes.\n",
        "\n",
        "* More Datasets. Apply the algorithm to more datasets on the UCI Machine Learning Repository.\n",
        "\n",
        "\n",
        "\n",
        "There several choices there is historically ID3 the Iterative Dichotomiser 3, C5 which I believe stands for Classifier as development of ID3. CART, which is for classification and regression tree. It's very common in Universal Choice. CHAID, which is for CHi-squared Automatic Interaction Detectior. MARS, which is multivariate adaptive regression splines. And the one that I'm going to be using Conditional Inference trees. Now decision trees in general have some pros and cons.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dv3kz8Cx6-bm",
        "colab_type": "text"
      },
      "source": [
        "##Methodology (100 ex comments)\n",
        "\n",
        "[code here]\n",
        "\n",
        "**Build and Train Data Models**\n",
        "\n",
        "* Comments connect the code to the algorithm steps\n",
        "* \n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JuFQehMoR0uz",
        "colab_type": "text"
      },
      "source": [
        "## Decision Tree Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P9F55nV5LWxV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## \n",
        "##    A2: Algorithm Implementation | 31005 | Advanced Data Analytics\n",
        "##    \n",
        "##    Authors: Rae Ho (13026998) & Damien Smith (13039957)\n",
        "##    Goals: - Implement Decision Tree Algorithm\n",
        "##           - Build & Train a model\n",
        "##           - Explore/Compare different parameters \n",
        "##    Code: Python 3\n",
        "##    Github: https://github.com/DamienSmith/UTS_ML2019_ID13039957/blob/master/A2_PracticalProject_13039957_13026998.ipynb\n",
        "##\n",
        "##"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ocOEBp3jxjcY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Import Libraries\n",
        "#numpy to work with arrays\n",
        "#panda for dataframe \n",
        "#sklearn to import iris dataset\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn import datasets"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EOIwEKy9xmGh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Get data\n",
        "#using iris data. Get it from sklearn dataset\n",
        "dataset = datasets.load_iris()\n",
        "\n",
        "# We want to turn the dataset into a panda dataframe\n",
        "df = pd.DataFrame(dataset['data'])\n",
        "\n",
        "\n",
        "# We also want to add the names of each feature to the dataframe\n",
        "df.columns = [name[:-5] for name in dataset['feature_names']]\n",
        "\n",
        "#We want to add the target to the dataframe too \n",
        "df['target'] = dataset['target']\n",
        "\n",
        "#this is to create a random sample of 30 entries\n",
        "testdf = df.sample(n=30, random_state=1).reset_index(drop=True)\n",
        "\n",
        "\n",
        "# We create target variable (into binary) in order to get a 'count' of all items in the dataframe (in case there are missing values) \n",
        "df['dummy'] = 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jp8GOuVhxrB6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#function to return the count of rows in the dataframe that match a condition\n",
        "\n",
        "def count_target(dframe, right, feature, value):\n",
        "  #this is to find the impurity to the left of and to the right of a slice on a cartesian plane   \n",
        "  if right:\n",
        "    cond = dframe[feature]>value\n",
        "  else:\n",
        "    cond = dframe[feature]<=value\n",
        "  \n",
        "  #this works by counting the total number of target items, 'mean' helps find probability of correct choice \n",
        "  #count = len(dframe[cond])\n",
        "  count = sum(cond)\n",
        "  return count\n",
        "\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AG-qabusxsxg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#We want to create a method to determine what the impurity is for any 'slice' of a feature. \n",
        "#We have decided to use the gini impurity which shows how likely it is that a randomly selected feature is correctly guessed\n",
        "\n",
        "##this takes the parameters:\n",
        "    #dframe which allows you to put in the dataframe you want to find the impurity for\n",
        "    #right which takes True or False to look at right or left impurity of a split point\n",
        "    #feature which is the feature (column) that you want to use/look at\n",
        "    #value which is the value that is to be split at\n",
        "    #target variable to be determined (which for the iris df we are using is 'target')\n",
        "\n",
        "\n",
        "#find gini impurity of a slice \n",
        "def gini_impurity_leaf(dframe, right, feature, value, target_variable):\n",
        "\n",
        "#this is to find the impurity to the left of and to the right of a slice on a cartesian plane   \n",
        "  if right:\n",
        "    cond = dframe[feature]>value\n",
        "  else:\n",
        "    cond = dframe[feature]<=value\n",
        "  \n",
        "  #this works by counting the total number of target items, 'mean' helps find probability of correct choice \n",
        "  count_of_target = dframe[cond].groupby(target_variable)['dummy'].count()\n",
        "  #print(\"count of target is: \")\n",
        "  #print(count_of_target)\n",
        "  #this uses the impurity formula of impurity = 1 - sum of (probability of being correct [count/length])^2\n",
        "  gini_impurity = 1 - (np.divide(count_of_target, len(dframe[cond])) ** 2).sum()\n",
        "  \n",
        "  return gini_impurity\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xuFwUzgBxxf2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#this finds the net gini impurity by adding the left and right slice\n",
        "# we want to find the 'slice' with the lowest net gini impurity \n",
        "\n",
        "def net_gini_impurity(dframe, feature, value, target_variable):  \n",
        "\n",
        "#  net_gini = gini_impurity_leaf(dframe, True, feature, value, target_variable) + gini_impurity_leaf(dframe, False, feature, value, target_variable) #this didn't work as it would have duplicate values or prefer edge cases\n",
        "# we had to use a weighted gini index\n",
        "\n",
        "  right_weight = count_target(dframe, True, feature, value) / len(dframe)\n",
        "  left_weight = 1 - right_weight\n",
        " #print(\"right weight is:{}\".format(right_weight))\n",
        "  net_gini = (right_weight * gini_impurity_leaf(dframe, True, feature, value, target_variable)) + (left_weight * gini_impurity_leaf(dframe, False, feature, value, target_variable))\n",
        "  \n",
        "  return net_gini\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KHx3uOwHxzHZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# This finds best split \n",
        "\n",
        "def find_best_split(dataframe):\n",
        "# Find the best split by going over every value in each feature and choosing the split with the lowest net impurity\n",
        "    lowest_impurity = 2  # keep track of the worst impurity\n",
        "    best_split = ['there is no split', 0]  # keep train of the feature / value that produced it\n",
        "\n",
        "    \n",
        "    for f in range(len(dataframe.columns) -2):  # loop through each feature --> the -2 is because it shouldnt look at TARGET and DUMMY\n",
        "        feat = dataframe.columns[f]             # store the name of the feature as 'feat'\n",
        "        for val in dataframe[feat].unique():  # loop through each unique value for that feature \n",
        "          #Calculate the net impurity of each value in feature\n",
        "          #print(feat,val)\n",
        "          net_imp = net_gini_impurity(dataframe, feat, val, 'target')\n",
        "          #print(feat,val,net_imp)\n",
        "          split = [feat, val]\n",
        "          # store best gain and best feature \n",
        "          if net_imp < lowest_impurity:\n",
        "            lowest_impurity = net_imp\n",
        "            best_split = split\n",
        "            #print(\"the current best split is:\")\n",
        "            #print(lowest_impurity,best_split)\n",
        "\n",
        "    return lowest_impurity, best_split"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "unGp9Ylcx1Cn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#this splits a dataframe to right and left sides\n",
        "\n",
        "def splitter(dframe):\n",
        "  impurity, featsplit = find_best_split(dframe)\n",
        "  feature = featsplit[0]\n",
        "  value = featsplit[1]\n",
        "  \n",
        "  #and store the values of those sides into a new data_frame  \n",
        "  right_split = dframe[dframe[feature]>value]\n",
        "  left_split =dframe[dframe[feature]<=value]      \n",
        "  \n",
        "  print(\"I have been split for the feature \", feature, \"at value\", value)\n",
        "  \n",
        "  return right_split, left_split, feature, value\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Anzk0Dzlx3W-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#This returns what the target is\n",
        "# This returns a predicted value for the terminal node\n",
        "# The returned value is the end value with the most common end result\n",
        "\n",
        "#def final_guess(feature, value, dframe, boolean=False):\n",
        "def final_guess(dframe):\n",
        "\n",
        "  dataset = dframe\n",
        "  targets = dataset.target\n",
        "  outcome = None\n",
        "  \n",
        "  outcome = targets.value_counts().idxmax()  #this is the first most common value in that set - so if it's a 50/50 split between two targets, it'll choose the first one.\n",
        "  \n",
        "  return outcome\n",
        "\n",
        "\n",
        "  #alternatively you could do 'outcomes = targets.mode()' but then you wont get a single int if there's multiple values"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lBvGZ4pwx_VX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#this creates an depth of arrays which we use to store values for later predict \n",
        "def createDepthArray(maxdepth):\n",
        "  for i in range(1,maxdepth):\n",
        "    depth = i\n",
        "    name = 'depth{}'.format(depth)          #get the name of the dict array\n",
        "\n",
        "    #populate the array with futher arrays\n",
        "    if name not in globals():       \n",
        "      globals()['depth%s' % depth] = {}       #create an array called depth#    \n",
        "    \n",
        "    if len(globals()['depth%s' % depth]) < 1:\n",
        "      for n in range(1, 2**depth + 1):              \n",
        "        globals()['depth%s' % depth][n] = {}\n",
        "      #elif name in globals():\n",
        "      #  print(\"already exists\")\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GMyXfP-pyE7R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#this helps insert values into the previous 'depth array' parts\n",
        "def insertNodeValue(depth, number, feature, value, side, guess):     \n",
        "  #number = getNumberPos(depth)\n",
        "  globals()['depth%s' % depth][number] = {'feature': feature, 'value': value, 'side': side, 'decision': guess}\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i8vbZDLuyLvH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def depthFiller(depth, outcome, number, maxdepth):\n",
        "#to fill subsequent depths's numbers that already has a final decision   \n",
        "  new_depth = depth+1\n",
        "  #number = getNumberPos(depth)\n",
        "  if new_depth < maxdepth:\n",
        "\n",
        "    print(\"i am at position {} and am filling lower depth\".format(number))\n",
        "    number_left = (number*2 - 1)\n",
        "    number_right = (number * 2)\n",
        "    globals()['depth%s' % new_depth][number_right] = {outcome}\n",
        "    globals()['depth%s' % new_depth][number_left] = {outcome}\n",
        "    #and then repopulate the sub-depths until max-depth\n",
        "    depthFiller(new_depth, outcome, number_right, maxdepth)\n",
        "    depthFiller(new_depth, outcome, number_left, maxdepth)\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wSYczhFayORy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#this returns the next empty number for that depth\n",
        "def getNumberPos(depth):\n",
        "  for i in range(1, 2**depth+1):\n",
        "    if len(globals()['depth%s' % depth][i]) == 0:\n",
        "      return i\n",
        "      break\n",
        "    else:\n",
        "      continue\n",
        "    \n",
        "  \n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7xrstiyZyRVE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#this is the main code that creates the tree by iteratively splitting\n",
        "#data at the best point\n",
        "\n",
        "def subsequent_split(dataframe, depth, maxdepth):\n",
        "  global maxDepth\n",
        "  maxDepth = maxdepth\n",
        "  createDepthArray(maxdepth)\n",
        "  \n",
        "  right_split, left_split, feature, value = splitter(dataframe)\n",
        "  #create a bunch of dict to max depth\n",
        "  \n",
        "\n",
        "  # store:  [number: {feature = '', value ='', side = 'left', decision = 0,1,3,null}]\n",
        "\n",
        "  if depth < maxdepth:\n",
        "    print(\"I am doing left side\")\n",
        "    \n",
        "    if left_split.empty:\n",
        "      print(\"depth is\", depth)\n",
        "      print(\"left side is empty\")\n",
        "\n",
        "           \n",
        "    #this currently looks for a unique value, and if only 1 \n",
        "    #consider doing a mininum impurity    \n",
        "    elif len(left_split['target'].unique()) < 2:\n",
        "      print(\"depth is\", depth)\n",
        "      print(\"i am now uniquely left\")\n",
        "      guess = final_guess(left_split)\n",
        "      print(\"final guess is {}\".format(guess))\n",
        "      number = getNumberPos(depth)\n",
        "      depthFiller(depth, guess, number, maxdepth)\n",
        "      insertNodeValue(depth, number, feature, value, 'left', guess)\n",
        "\n",
        "\n",
        "    else:  \n",
        "      #print(len(left_split['target'].unique()))\n",
        "      print(\"depth is\", depth)\n",
        "      print(\"i have split the left side at {}{}\".format(feature, value))\n",
        "      number = getNumberPos(depth)\n",
        "      insertNodeValue(depth, number, feature, value, 'left', None)\n",
        "      subsequent_split(left_split,depth+1,maxdepth)\n",
        "     \n",
        "    print(\"I am doing right side\")\n",
        "    if right_split.empty:\n",
        "      print(\"depth is\", depth)\n",
        "      print(\"right side is empty\")  \n",
        "    elif len(right_split['target'].unique()) < 2: \n",
        "      print(\"depth is\", depth)\n",
        "      print(\"i am now uniquely right\")\n",
        "      guess = final_guess(right_split)\n",
        "      print(\"final guess is {}\".format(guess))\n",
        "      number = getNumberPos(depth)\n",
        "      depthFiller(depth, guess, number, maxdepth)\n",
        "      insertNodeValue(depth, number, feature, value, 'right', guess)\n",
        "            \n",
        "    else:\n",
        "      print(\"depth is\", depth)\n",
        "      print(\"I have split the right side at {}{}\".format(feature, value))\n",
        "      number = getNumberPos(depth)\n",
        "      insertNodeValue(depth, number, feature, value, 'right', None)\n",
        "      subsequent_split(right_split,depth+1,maxdepth)\n",
        "      \n",
        "      \n",
        "   \n",
        "   \n",
        " "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GI7Zbsh9yah6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#def to clear the dictionary. Needed if we want to re-train/subsequent_split  \n",
        "def clearTrainedData():\n",
        "  for n in range(1, maxDepth):              \n",
        "    globals()['depth%s' % n].clear()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f1u4v5VnyfOq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#this is for subsequent predictions after the first depth\n",
        "def subsequentPredict(test_df_row, number, depth):\n",
        "  index = number\n",
        "  depth = depth + 1\n",
        "  if depth < maxDepth:\n",
        "    for x in test_df_row.columns:               #for each df header            \n",
        "      if globals()['depth%s' % depth][index]['feature'] == x:           #to see if feature is same\n",
        "        dictVal = globals()['depth%s' % depth][index]['value']                # value of depth1      \n",
        "        for n in range(len(test_df_row[x])):     #loop each index row in testdf        \n",
        "          rowVal = test_df_row.loc[n, x ]   #store the value for that row\n",
        "      \n",
        "          if globals()['depth%s' % depth][index]['side'] == 'right':      # if depth entry is left or right\n",
        "            if rowVal > dictVal:\n",
        "              if globals()['depth%s' % depth][index]['decision'] is not None:\n",
        "                print(\"the prediction is {}\".format(globals()['depth%s' % depth][index]['decision']))\n",
        "              else:\n",
        "                #print(\"need to continue\")\n",
        "                subsequentPredict(test_df_row, index*2 - 1, depth)\n",
        "                subsequentPredict(test_df_row, index*2, depth)\n",
        "                \n",
        "          if globals()['depth%s' % depth][index]['side'] == 'left':      # if depth entry is left or right   \n",
        "            if rowVal <= dictVal:\n",
        "              if globals()['depth%s' % depth][index]['decision'] is not None:\n",
        "                print(\"the prediction is {}\".format(globals()['depth%s' % depth][index]['decision']))\n",
        "              else:\n",
        "                #print(\"need to continue\")\n",
        "                subsequentPredict(test_df_row, index*2 - 1, depth)\n",
        "                subsequentPredict(test_df_row, index*2, depth)\n",
        "  #else:\n",
        "  #  print(\"reached max depth already\")\n",
        "    \n",
        "\n",
        "            \n",
        "            \n",
        "           \n",
        "        \n",
        "     \n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GR8PJ2OOyj1K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def firstPredict(test_df_row):\n",
        "  for x in test_df_row.columns:               #for each df header\n",
        "    for i in range(1,len(depth1)+1):          #loop through each dict key\n",
        "      if depth1[i]['feature'] == x:           #to see if feature is same\n",
        "        #print(\"feature to look over is{}\".format(x))\n",
        "        dictVal = depth1[i]['value']                # value of depth1\n",
        "        #print(\"dictval is {}\".format(dictVal))\n",
        "        #print(type(dictVal))\n",
        "        \n",
        "        for n in range(len(test_df_row[x])):     #loop each index row in testdf        \n",
        "          rowVal = test_df_row.loc[n, x ]   #store the value for that row\n",
        "          #print(\"rowVal is {}\".format(rowVal))\n",
        "          #print(type(rowVal))\n",
        "          #print(depth1[i]['side'])\n",
        "          \n",
        "          if depth1[i]['side'] == 'right':      # if depth1 entry is left or right\n",
        "            if rowVal > dictVal:\n",
        "              #print(\"Row Val is greater than DictVal\")\n",
        "              if depth1[i]['decision'] is not None:\n",
        "                print(\"the prediction is {}\".format(depth1[i]['decision']))\n",
        "              elif depth1[i]['decision'] is None:\n",
        "                #print(\"need to continue\")\n",
        "                subsequentPredict(test_df_row, i*2 - 1, 1)\n",
        "                subsequentPredict(test_df_row, i*2, 1)\n",
        "            #else:\n",
        "            #  print(\"rowVal does not match this node\")\n",
        "          \n",
        "          if depth1[i]['side'] == 'left':      # if depth1 entry is left or right   \n",
        "            if rowVal <= dictVal:\n",
        "              #print(\"Row Val is less than DictVal\")\n",
        "              if depth1[i]['decision'] is not None:\n",
        "                print(\"the prediction is {}\".format(depth1[i]['decision']))\n",
        "              elif depth1[i]['decision'] is None:\n",
        "                #print(\"need to continue\")\n",
        "                subsequentPredict(test_df_row, i*2 - 1, 1)\n",
        "                subsequentPredict(test_df_row, i*2, 1)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PtZSHKf3ypOP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#this is to test each row of the inputed dataframe\n",
        "def testData(dataframe):\n",
        "  testdata = dataframe\n",
        "  \n",
        "  for i in range(len(testdf)):\n",
        "    testrow = testdata[:1]\n",
        "    print(\"for the row:\")\n",
        "    print(testrow.to_string(index=False))\n",
        "    firstPredict(testrow)\n",
        "    testdata = testdata.drop([0,0]).reset_index(drop=True)\n",
        "    print(\"\\n\")\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KY1ECmnUywNL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "####################################################\n",
        "#######this is where we finally call things ########\n",
        "####################################################"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TFsEuNs6y6H0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#this is to build the tree\n",
        "#the first parameter is the training dataframe\n",
        "#the second parameter is the depth to start at, which is always 1\n",
        "#the third parameter is the max depth you want to trim the decision tree to\n",
        "subsequent_split(df, 1, 5)    #this is max depth 5\n",
        "\n",
        "#You will need to clear the training data if you want to 'train' different data\n",
        "#clearTrainedData()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1l_bgtS0y6Gu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#this tests the data you are using\n",
        "testData(testdf)\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ISzhqah26-KQ",
        "colab_type": "text"
      },
      "source": [
        "##Evaluation (200)\n",
        "\n",
        "\n",
        "**Report Execution on Data**\n",
        "\n",
        "**Report Testing**\n",
        "\n",
        "**Efficiency Analysis**\n",
        "\n",
        "**Comparative Study**\n",
        "\n",
        "ideas:\n",
        "* how does each feature distribution look like? Are there any differences between feature distribution in train and test data?\n",
        "* are there any meaningful interactions between the features?\n",
        "* are there outliers and can they be explained?\n",
        "* are there missing values or duplicates? What are reasons for them?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G-FTRF_s85K_",
        "colab_type": "text"
      },
      "source": [
        "## Conclusion (100)\n",
        "\n",
        "\n",
        "Discuss Reflections\n",
        "\n",
        "Propose Possible Improvements\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gLV7jwde8-oX",
        "colab_type": "text"
      },
      "source": [
        "## Ethical (200)\n",
        "\n",
        "* Discuss the social/ethical aspect of the project\n",
        "  * adopt an ethical model (e.g. Ultitarian or Kantian)\n",
        "* Consider how the technique could be misused"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TdspPnHH9Cn3",
        "colab_type": "text"
      },
      "source": [
        "## Video Pitch\n",
        "\n",
        "[url to video]\n",
        "\n",
        "**Highlight Challenges and Effort**\n",
        "\n",
        "* Describe challenges and how the team addressed them.\n",
        "  * Python Skills\n",
        "    * Pandas, numpy, others?\n",
        "    * Dictionary (Hashmap)\n",
        "  * Code structure\n",
        "    * Generating 'depth' and 'terminal' flags in dictionary\n",
        "    * Handling looping (to avoid hardcoding for iris and adding flexibility to use any dataframe)\n",
        "  * Managing Assignment scope \n",
        "    * We initially wanted to run each cost function (ID3, CART and C4.5) (entropy? info gain?)\n",
        "    * We also wanted to turn it into a Random forest\n",
        "    * scoping this down into the time we had available was a challenge (while learning)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NtZqh3f9zW9b",
        "colab_type": "text"
      },
      "source": [
        "## References\n",
        "\n",
        "Brownlee, J., 2016. Master Machine Learning Algorithms: discover how they work and implement them from scratch. Machine Learning Mastery.\n",
        "\n",
        "McKinney, W., 2011. pandas: a foundational Python library for data analysis and statistics. Python for High Performance and Scientific Computing, 14.\n",
        "\n",
        "Peng, W., Chen, J. and Zhou, H., 2009. An implementation of ID3-decision tree learning algorithm. From web.arch.usyd.edu.au/wpeng/DecisionTree2.pdf viewed 21/09/2019\n",
        "\n",
        "Singh, S. & Gupta, P., 2014. Comparative study ID3, cart and C4. 5 decision tree algorithm: a survey. International Journal of Advanced Information Science and Technology (IJAIST), 27(27), pp.97-103.\n"
      ]
    }
  ]
}